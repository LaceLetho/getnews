"""
æŠ¥å‘Šæ ¼å¼å®Œæ•´æ€§å±æ€§æµ‹è¯•

ä½¿ç”¨Hypothesisè¿›è¡Œå±æ€§æµ‹è¯•ï¼ŒéªŒè¯æŠ¥å‘Šç”Ÿæˆå™¨çš„æ ¼å¼å®Œæ•´æ€§ã€‚
**åŠŸèƒ½: crypto-news-analyzer, å±æ€§ 6: æŠ¥å‘Šæ ¼å¼å®Œæ•´æ€§**
**éªŒè¯: éœ€æ±‚ 7.1, 7.4**
"""

import pytest
import re
from datetime import datetime, timedelta
from hypothesis import given, strategies as st, assume, settings
from typing import Dict, List, Optional, Any

from crypto_news_analyzer.models import ContentItem, CrawlStatus, CrawlResult, AnalysisResult
from crypto_news_analyzer.reporters.report_generator import (
    ReportGenerator, 
    AnalyzedData, 
    create_analyzed_data,
    validate_report_data
)


# ç­–ç•¥å®šä¹‰ï¼šç”Ÿæˆæµ‹è¯•æ•°æ®
@st.composite
def valid_content_item(draw):
    """ç”Ÿæˆæœ‰æ•ˆçš„ContentItem"""
    # ç”Ÿæˆå”¯ä¸€ID
    import time
    unique_id = f"test_{draw(st.integers(min_value=1, max_value=999999))}_{int(time.time() * 1000000) % 1000000}"
    
    # ç”Ÿæˆæ—¶é—´ï¼ˆæœ€è¿‘72å°æ—¶å†…ï¼‰
    now = datetime.now()
    hours_ago = draw(st.integers(min_value=0, max_value=72))
    publish_time = now - timedelta(hours=hours_ago)
    
    # ç”Ÿæˆå†…å®¹
    title = draw(st.text(min_size=5, max_size=100, alphabet=st.characters(whitelist_categories=('Lu', 'Ll', 'Nd', 'Zs', 'Po'))))
    content = draw(st.text(min_size=10, max_size=500, alphabet=st.characters(whitelist_categories=('Lu', 'Ll', 'Nd', 'Zs', 'Po'))))
    
    # ç¡®ä¿æ ‡é¢˜å’Œå†…å®¹ä¸ä¸ºç©º
    assume(title.strip())
    assume(content.strip())
    
    # ç”ŸæˆURL
    url_id = draw(st.integers(min_value=1, max_value=999999))
    url = f"https://example.com/news/{url_id}"
    
    source_name = draw(st.sampled_from(["æµ‹è¯•RSSæº", "æµ‹è¯•Xæº", "æµ‹è¯•APIæº", "æ–°é—»æºA", "æ–°é—»æºB"]))
    source_type = draw(st.sampled_from(["rss", "x", "rest_api"]))
    
    return ContentItem(
        id=unique_id,
        title=title.strip(),
        content=content.strip(),
        url=url,
        publish_time=publish_time,
        source_name=source_name,
        source_type=source_type
    )


@st.composite
def valid_analysis_result(draw, content_id: str):
    """ç”Ÿæˆæœ‰æ•ˆçš„AnalysisResult"""
    categories = [
        "å¤§æˆ·åŠ¨å‘", "åˆ©ç‡äº‹ä»¶", "ç¾å›½æ”¿åºœç›‘ç®¡æ”¿ç­–", 
        "å®‰å…¨äº‹ä»¶", "æ–°äº§å“", "å¸‚åœºæ–°ç°è±¡", "æœªåˆ†ç±»", "å¿½ç•¥"
    ]
    
    category = draw(st.sampled_from(categories))
    confidence = draw(st.floats(min_value=0.0, max_value=1.0))
    reasoning = draw(st.text(min_size=5, max_size=200, alphabet=st.characters(whitelist_categories=('Lu', 'Ll', 'Nd', 'Zs', 'Po'))))
    should_ignore = draw(st.booleans())
    
    # å¦‚æœåˆ†ç±»æ˜¯"å¿½ç•¥"ï¼Œshould_ignoreåº”è¯¥ä¸ºTrue
    if category == "å¿½ç•¥":
        should_ignore = True
    
    # ç”Ÿæˆå…³é”®ç‚¹
    key_points = draw(st.lists(
        st.text(min_size=3, max_size=50, alphabet=st.characters(whitelist_categories=('Lu', 'Ll', 'Nd', 'Zs', 'Po'))),
        min_size=0,
        max_size=5
    ))
    # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
    key_points = [point.strip() for point in key_points if point.strip()]
    
    return AnalysisResult(
        content_id=content_id,
        category=category,
        confidence=confidence,
        reasoning=reasoning.strip() if reasoning.strip() else "é»˜è®¤åˆ†æç†ç”±",
        should_ignore=should_ignore,
        key_points=key_points
    )


@st.composite
def valid_crawl_result(draw):
    """ç”Ÿæˆæœ‰æ•ˆçš„CrawlResult"""
    source_name = draw(st.sampled_from(["RSSæºA", "RSSæºB", "XæºA", "XæºB", "APIæºA"]))
    status = draw(st.sampled_from(["success", "error"]))
    item_count = draw(st.integers(min_value=0, max_value=100))
    
    error_message = None
    if status == "error":
        error_messages = [
            "ç½‘ç»œè¿æ¥è¶…æ—¶", "RSSè§£æå¤±è´¥", "è®¤è¯å¤±è´¥", 
            "APIé™åˆ¶", "æœåŠ¡ä¸å¯ç”¨", "æ•°æ®æ ¼å¼é”™è¯¯"
        ]
        error_message = draw(st.sampled_from(error_messages))
    
    return CrawlResult(
        source_name=source_name,
        status=status,
        item_count=item_count,
        error_message=error_message
    )


@st.composite
def valid_crawl_status(draw):
    """ç”Ÿæˆæœ‰æ•ˆçš„CrawlStatus"""
    # ç”ŸæˆRSSç»“æœ
    rss_results = draw(st.lists(valid_crawl_result(), min_size=0, max_size=5))
    
    # ç”ŸæˆXç»“æœ
    x_results = draw(st.lists(valid_crawl_result(), min_size=0, max_size=5))
    
    # è®¡ç®—æ€»é¡¹ç›®æ•°
    total_items = sum(result.item_count for result in rss_results + x_results)
    
    # ç”Ÿæˆæ‰§è¡Œæ—¶é—´
    execution_time = datetime.now() - timedelta(minutes=draw(st.integers(min_value=0, max_value=60)))
    
    return CrawlStatus(
        rss_results=rss_results,
        x_results=x_results,
        total_items=total_items,
        execution_time=execution_time
    )


@st.composite
def valid_analyzed_data(draw):
    """ç”Ÿæˆæœ‰æ•ˆçš„AnalyzedData"""
    # ç”Ÿæˆå†…å®¹é¡¹
    content_items = draw(st.lists(valid_content_item(), min_size=0, max_size=20))
    
    # æŒ‰ç±»åˆ«åˆ†ç»„
    categories = [
        "å¤§æˆ·åŠ¨å‘", "åˆ©ç‡äº‹ä»¶", "ç¾å›½æ”¿åºœç›‘ç®¡æ”¿ç­–", 
        "å®‰å…¨äº‹ä»¶", "æ–°äº§å“", "å¸‚åœºæ–°ç°è±¡", "æœªåˆ†ç±»"
    ]
    
    categorized_items = {}
    analysis_results = {}
    
    for category in categories:
        categorized_items[category] = []
    
    # éšæœºåˆ†é…å†…å®¹é¡¹åˆ°ç±»åˆ«
    for item in content_items:
        category = draw(st.sampled_from(categories))
        categorized_items[category].append(item)
        
        # ç”Ÿæˆå¯¹åº”çš„åˆ†æç»“æœ
        analysis_result = draw(valid_analysis_result(item.id))
        # ç¡®ä¿åˆ†æç»“æœçš„åˆ†ç±»ä¸åˆ†ç»„ä¸€è‡´
        analysis_result.category = category
        analysis_results[item.id] = analysis_result
    
    # ç”Ÿæˆæ—¶é—´çª—å£
    time_window_hours = draw(st.integers(min_value=1, max_value=72))
    reference_time = datetime.now()
    
    return create_analyzed_data(
        categorized_items=categorized_items,
        analysis_results=analysis_results,
        time_window_hours=time_window_hours,
        reference_time=reference_time
    )


class TestReportFormatCompletenessProperties:
    """æŠ¥å‘Šæ ¼å¼å®Œæ•´æ€§å±æ€§æµ‹è¯•"""
    
    def setup_method(self):
        """æµ‹è¯•å‰è®¾ç½®"""
        self.generator = ReportGenerator(include_summary=True)
    
    @given(
        analyzed_data=valid_analyzed_data(),
        crawl_status=valid_crawl_status()
    )
    @settings(max_examples=100, deadline=None)
    def test_report_format_completeness(self, analyzed_data, crawl_status):
        """
        å±æ€§æµ‹è¯•ï¼šæŠ¥å‘Šæ ¼å¼å®Œæ•´æ€§
        
        **åŠŸèƒ½: crypto-news-analyzer, å±æ€§ 6: æŠ¥å‘Šæ ¼å¼å®Œæ•´æ€§**
        **éªŒè¯: éœ€æ±‚ 7.1, 7.4**
        
        å¯¹äºä»»ä½•ç”Ÿæˆçš„æŠ¥å‘Šï¼Œåº”è¯¥åŒ…å«æ—¶é—´çª—å£ä¿¡æ¯çš„å¤´éƒ¨ã€æ•°æ®æºçŠ¶æ€è¡¨æ ¼ï¼Œä»¥åŠæ¯æ¡ä¿¡æ¯çš„åŸæ–‡é“¾æ¥
        """
        # ç”ŸæˆæŠ¥å‘Š
        report = self.generator.generate_report(analyzed_data, crawl_status)
        
        # éªŒè¯ï¼šæŠ¥å‘Šä¸ä¸ºç©º
        assert report and report.strip(), "ç”Ÿæˆçš„æŠ¥å‘Šä¸èƒ½ä¸ºç©º"
        
        # éªŒè¯éœ€æ±‚7.1ï¼šåŒ…å«æ—¶é—´çª—å£ä¿¡æ¯çš„æŠ¥å‘Šå¤´éƒ¨
        self._verify_report_header(report, analyzed_data)
        
        # éªŒè¯éœ€æ±‚7.2ï¼šåŒ…å«æ•°æ®æºçŠ¶æ€è¡¨æ ¼
        self._verify_status_table(report, crawl_status)
        
        # éªŒè¯éœ€æ±‚7.3ï¼šæŒ‰ç±»åˆ«ç»„ç»‡åˆ†æç»“æœ
        self._verify_category_sections(report, analyzed_data)
        
        # éªŒè¯éœ€æ±‚7.4ï¼šæ¯æ¡ä¿¡æ¯åŒ…å«åŸæ–‡é“¾æ¥
        self._verify_original_links(report, analyzed_data)
        
        # éªŒè¯éœ€æ±‚7.6ï¼šä½¿ç”¨Markdownæ ¼å¼
        self._verify_markdown_format(report, analyzed_data)
        
        # éªŒè¯éœ€æ±‚7.7ï¼šç©ºç±»åˆ«æ˜¾ç¤ºä¸ºç©º
        self._verify_empty_categories(report, analyzed_data)
    
    def _verify_report_header(self, report: str, data: AnalyzedData):
        """éªŒè¯æŠ¥å‘Šå¤´éƒ¨åŒ…å«æ—¶é—´çª—å£ä¿¡æ¯"""
        # éªŒè¯æ ‡é¢˜å­˜åœ¨
        assert "# åŠ å¯†è´§å¸æ–°é—»åˆ†ææŠ¥å‘Š" in report, "æŠ¥å‘Šåº”è¯¥åŒ…å«ä¸»æ ‡é¢˜"
        
        # éªŒè¯æŠ¥å‘Šä¿¡æ¯éƒ¨åˆ†å­˜åœ¨
        assert "## æŠ¥å‘Šä¿¡æ¯" in report, "æŠ¥å‘Šåº”è¯¥åŒ…å«æŠ¥å‘Šä¿¡æ¯éƒ¨åˆ†"
        
        # éªŒè¯ç”Ÿæˆæ—¶é—´å­˜åœ¨
        assert "**ç”Ÿæˆæ—¶é—´**:" in report, "æŠ¥å‘Šåº”è¯¥åŒ…å«ç”Ÿæˆæ—¶é—´"
        
        # éªŒè¯æ—¶é—´çª—å£ä¿¡æ¯å­˜åœ¨
        time_window_pattern = rf"\*\*æ•°æ®æ—¶é—´çª—å£\*\*:\s*{data.time_window_hours}\s*å°æ—¶"
        assert re.search(time_window_pattern, report), f"æŠ¥å‘Šåº”è¯¥åŒ…å«æ—¶é—´çª—å£ä¿¡æ¯: {data.time_window_hours} å°æ—¶"
        
        # éªŒè¯æ—¶é—´èŒƒå›´å­˜åœ¨
        assert "**æ•°æ®æ—¶é—´èŒƒå›´**:" in report, "æŠ¥å‘Šåº”è¯¥åŒ…å«æ•°æ®æ—¶é—´èŒƒå›´"
        
        # éªŒè¯æ—¶é—´æ ¼å¼æ­£ç¡®ï¼ˆYYYY-MM-DD HH:MM:SSï¼‰
        time_format_pattern = r"\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}"
        time_matches = re.findall(time_format_pattern, report)
        assert len(time_matches) >= 3, "æŠ¥å‘Šåº”è¯¥åŒ…å«è‡³å°‘3ä¸ªæ—¶é—´æˆ³ï¼ˆç”Ÿæˆæ—¶é—´ã€å¼€å§‹æ—¶é—´ã€ç»“æŸæ—¶é—´ï¼‰"
    
    def _verify_status_table(self, report: str, status: CrawlStatus):
        """éªŒè¯æ•°æ®æºçŠ¶æ€è¡¨æ ¼"""
        # éªŒè¯çŠ¶æ€è¡¨æ ¼æ ‡é¢˜å­˜åœ¨
        assert "## æ•°æ®æºçˆ¬å–çŠ¶æ€" in report, "æŠ¥å‘Šåº”è¯¥åŒ…å«æ•°æ®æºçŠ¶æ€è¡¨æ ¼æ ‡é¢˜"
        
        # éªŒè¯è¡¨æ ¼å¤´éƒ¨å­˜åœ¨
        table_headers = ["æ•°æ®æºç±»å‹", "æ•°æ®æºåç§°", "çŠ¶æ€", "è·å–æ•°é‡", "é”™è¯¯ä¿¡æ¯"]
        for header in table_headers:
            assert header in report, f"çŠ¶æ€è¡¨æ ¼åº”è¯¥åŒ…å«åˆ—å¤´: {header}"
        
        # éªŒè¯è¡¨æ ¼åˆ†éš”ç¬¦å­˜åœ¨
        assert "|-----------|-----------|------|----------|----------|" in report, "çŠ¶æ€è¡¨æ ¼åº”è¯¥åŒ…å«åˆ†éš”ç¬¦"
        
        # éªŒè¯RSSæºçŠ¶æ€
        for rss_result in status.rss_results:
            assert rss_result.source_name in report, f"æŠ¥å‘Šåº”è¯¥åŒ…å«RSSæº: {rss_result.source_name}"
            assert "RSS" in report, "æŠ¥å‘Šåº”è¯¥æ ‡è¯†RSSæ•°æ®æºç±»å‹"
            
            # éªŒè¯çŠ¶æ€å›¾æ ‡
            if rss_result.status == "success":
                assert "âœ…" in report, "æˆåŠŸçŠ¶æ€åº”è¯¥æœ‰å¯¹åº”å›¾æ ‡"
            else:
                assert "âŒ" in report, "å¤±è´¥çŠ¶æ€åº”è¯¥æœ‰å¯¹åº”å›¾æ ‡"
            
            # éªŒè¯è·å–æ•°é‡
            assert str(rss_result.item_count) in report, f"æŠ¥å‘Šåº”è¯¥åŒ…å«è·å–æ•°é‡: {rss_result.item_count}"
        
        # éªŒè¯XæºçŠ¶æ€
        for x_result in status.x_results:
            assert x_result.source_name in report, f"æŠ¥å‘Šåº”è¯¥åŒ…å«Xæº: {x_result.source_name}"
            assert "X/Twitter" in report, "æŠ¥å‘Šåº”è¯¥æ ‡è¯†X/Twitteræ•°æ®æºç±»å‹"
        
        # éªŒè¯æ±‡æ€»ä¿¡æ¯
        assert "**æ±‡æ€»**" in report, "çŠ¶æ€è¡¨æ ¼åº”è¯¥åŒ…å«æ±‡æ€»è¡Œ"
        assert str(status.total_items) in report, f"æŠ¥å‘Šåº”è¯¥åŒ…å«æ€»é¡¹ç›®æ•°: {status.total_items}"
        
        success_count = status.get_success_count()
        error_count = status.get_error_count()
        assert f"{success_count} æˆåŠŸ" in report, f"æŠ¥å‘Šåº”è¯¥æ˜¾ç¤ºæˆåŠŸæ•°é‡: {success_count}"
        assert f"{error_count} å¤±è´¥" in report, f"æŠ¥å‘Šåº”è¯¥æ˜¾ç¤ºå¤±è´¥æ•°é‡: {error_count}"
    
    def _verify_category_sections(self, report: str, data: AnalyzedData):
        """éªŒè¯åˆ†ç±»éƒ¨åˆ†ç»„ç»‡"""
        # å®šä¹‰é¢„æœŸçš„ç±»åˆ«å’Œå›¾æ ‡
        expected_categories = [
            ("å¤§æˆ·åŠ¨å‘", "ğŸ‹"),
            ("åˆ©ç‡äº‹ä»¶", "ğŸ“ˆ"),
            ("ç¾å›½æ”¿åºœç›‘ç®¡æ”¿ç­–", "ğŸ›ï¸"),
            ("å®‰å…¨äº‹ä»¶", "ğŸ”’"),
            ("æ–°äº§å“", "ğŸš€"),
            ("å¸‚åœºæ–°ç°è±¡", "ğŸ“Š"),
            ("æœªåˆ†ç±»", "â“")
        ]
        
        # éªŒè¯æ¯ä¸ªç±»åˆ«éƒ¨åˆ†éƒ½å­˜åœ¨
        for category_name, emoji in expected_categories:
            category_header = f"## {emoji} {category_name}"
            assert category_header in report, f"æŠ¥å‘Šåº”è¯¥åŒ…å«åˆ†ç±»éƒ¨åˆ†: {category_header}"
            
            # è·å–è¯¥ç±»åˆ«çš„å†…å®¹é¡¹
            items = data.categorized_items.get(category_name, [])
            
            if items:
                # å¦‚æœæœ‰å†…å®¹ï¼ŒéªŒè¯å†…å®¹æ•°é‡æ˜¾ç¤º
                count_pattern = rf"\*å…± {len(items)} æ¡ç›¸å…³å†…å®¹\*"
                assert re.search(count_pattern, report), f"æœ‰å†…å®¹çš„åˆ†ç±»åº”è¯¥æ˜¾ç¤ºå†…å®¹æ•°é‡: {len(items)}"
                
                # éªŒè¯æ¯ä¸ªå†…å®¹é¡¹éƒ½æœ‰æ ‡é¢˜
                for item in items:
                    # å†…å®¹é¡¹æ ‡é¢˜åº”è¯¥ä»¥æ•°å­—å¼€å¤´
                    title_pattern = rf"### \d+\. {re.escape(item.title)}"
                    assert re.search(title_pattern, report), f"æŠ¥å‘Šåº”è¯¥åŒ…å«å†…å®¹é¡¹æ ‡é¢˜: {item.title}"
            else:
                # å¦‚æœæ²¡æœ‰å†…å®¹ï¼ŒéªŒè¯ç©ºå†…å®¹æç¤º
                assert "*æœ¬æ—¶é—´çª—å£å†…æš‚æ— ç›¸å…³å†…å®¹*" in report, f"ç©ºåˆ†ç±» {category_name} åº”è¯¥æ˜¾ç¤ºæ— å†…å®¹æç¤º"
    
    def _verify_original_links(self, report: str, data: AnalyzedData):
        """éªŒè¯æ¯æ¡ä¿¡æ¯åŒ…å«åŸæ–‡é“¾æ¥"""
        # æ”¶é›†æ‰€æœ‰å†…å®¹é¡¹
        all_items = []
        for items in data.categorized_items.values():
            all_items.extend(items)
        
        # éªŒè¯æ¯ä¸ªå†…å®¹é¡¹éƒ½æœ‰åŸæ–‡é“¾æ¥
        for item in all_items:
            # éªŒè¯é“¾æ¥æ ¼å¼ï¼š[æŸ¥çœ‹åŸæ–‡](URL)
            link_pattern = rf"\[æŸ¥çœ‹åŸæ–‡\]\({re.escape(item.url)}\)"
            assert re.search(link_pattern, report), f"æŠ¥å‘Šåº”è¯¥åŒ…å«åŸæ–‡é“¾æ¥: {item.url}"
            
            # éªŒè¯é“¾æ¥éƒ¨åˆ†çš„æ ‡ç­¾
            assert "**é“¾æ¥**:" in report, "æŠ¥å‘Šåº”è¯¥åŒ…å«é“¾æ¥æ ‡ç­¾"
    
    def _verify_markdown_format(self, report: str, data: AnalyzedData):
        """éªŒè¯Markdownæ ¼å¼"""
        # éªŒè¯æ ‡é¢˜æ ¼å¼
        assert re.search(r"^# ", report, re.MULTILINE), "æŠ¥å‘Šåº”è¯¥åŒ…å«ä¸€çº§æ ‡é¢˜"
        assert re.search(r"^## ", report, re.MULTILINE), "æŠ¥å‘Šåº”è¯¥åŒ…å«äºŒçº§æ ‡é¢˜"
        
        # åªæœ‰å½“æœ‰å†…å®¹é¡¹æ—¶æ‰éªŒè¯ä¸‰çº§æ ‡é¢˜
        total_items = sum(len(items) for items in data.categorized_items.values())
        if total_items > 0:
            assert re.search(r"^### ", report, re.MULTILINE), "æœ‰å†…å®¹æ—¶æŠ¥å‘Šåº”è¯¥åŒ…å«ä¸‰çº§æ ‡é¢˜"
        
        # éªŒè¯ç²—ä½“æ ¼å¼
        assert re.search(r"\*\*[^*]+\*\*", report), "æŠ¥å‘Šåº”è¯¥åŒ…å«ç²—ä½“æ–‡æœ¬"
        
        # éªŒè¯æ–œä½“æ ¼å¼
        assert re.search(r"\*[^*]+\*", report), "æŠ¥å‘Šåº”è¯¥åŒ…å«æ–œä½“æ–‡æœ¬"
        
        # éªŒè¯è¡¨æ ¼æ ¼å¼
        assert re.search(r"\|.*\|", report), "æŠ¥å‘Šåº”è¯¥åŒ…å«è¡¨æ ¼"
        
        # åªæœ‰å½“æœ‰å†…å®¹é¡¹æ—¶æ‰éªŒè¯é“¾æ¥æ ¼å¼
        if total_items > 0:
            assert re.search(r"\[.*\]\(.*\)", report), "æœ‰å†…å®¹æ—¶æŠ¥å‘Šåº”è¯¥åŒ…å«é“¾æ¥"
    
    def _verify_empty_categories(self, report: str, data: AnalyzedData):
        """éªŒè¯ç©ºç±»åˆ«æ˜¾ç¤º"""
        for category_name, items in data.categorized_items.items():
            if not items:
                # ç©ºç±»åˆ«åº”è¯¥æ˜¾ç¤ºæ— å†…å®¹æç¤º
                empty_message = "*æœ¬æ—¶é—´çª—å£å†…æš‚æ— ç›¸å…³å†…å®¹*"
                
                # æŸ¥æ‰¾è¯¥ç±»åˆ«éƒ¨åˆ†
                category_patterns = [
                    f"## ğŸ‹ {category_name}",
                    f"## ğŸ“ˆ {category_name}",
                    f"## ğŸ›ï¸ {category_name}",
                    f"## ğŸ”’ {category_name}",
                    f"## ğŸš€ {category_name}",
                    f"## ğŸ“Š {category_name}",
                    f"## â“ {category_name}"
                ]
                
                category_found = False
                for pattern in category_patterns:
                    if pattern in report:
                        category_found = True
                        # åœ¨è¯¥ç±»åˆ«éƒ¨åˆ†ä¹‹ååº”è¯¥æœ‰ç©ºå†…å®¹æç¤º
                        category_index = report.find(pattern)
                        next_section_index = report.find("## ", category_index + len(pattern))
                        if next_section_index == -1:
                            next_section_index = len(report)
                        
                        category_section = report[category_index:next_section_index]
                        assert empty_message in category_section, (
                            f"ç©ºåˆ†ç±» {category_name} åº”è¯¥åœ¨å…¶éƒ¨åˆ†ä¸­æ˜¾ç¤ºæ— å†…å®¹æç¤º"
                        )
                        break
                
                if not category_found:
                    # å¦‚æœç±»åˆ«éƒ¨åˆ†ä¸å­˜åœ¨ï¼Œè¿™ä¹Ÿæ˜¯å¯ä»¥æ¥å—çš„ï¼ˆå¯èƒ½è¢«ä¼˜åŒ–æ‰äº†ï¼‰
                    pass
    
    @given(
        analyzed_data=valid_analyzed_data(),
        crawl_status=valid_crawl_status()
    )
    @settings(max_examples=50, deadline=None)
    def test_report_content_structure(self, analyzed_data, crawl_status):
        """
        å±æ€§æµ‹è¯•ï¼šæŠ¥å‘Šå†…å®¹ç»“æ„å®Œæ•´æ€§
        
        éªŒè¯æŠ¥å‘Šçš„æ•´ä½“ç»“æ„å’Œå†…å®¹ç»„ç»‡ç¬¦åˆè¦æ±‚
        """
        report = self.generator.generate_report(analyzed_data, crawl_status)
        
        # éªŒè¯æŠ¥å‘Šç»“æ„çš„é¡ºåº
        sections = [
            "# åŠ å¯†è´§å¸æ–°é—»åˆ†ææŠ¥å‘Š",
            "## æŠ¥å‘Šä¿¡æ¯",
            "## æ•°æ®æºçˆ¬å–çŠ¶æ€",
            "## ğŸ‹ å¤§æˆ·åŠ¨å‘",
            "## ğŸ“ˆ åˆ©ç‡äº‹ä»¶",
            "## ğŸ›ï¸ ç¾å›½æ”¿åºœç›‘ç®¡æ”¿ç­–",
            "## ğŸ”’ å®‰å…¨äº‹ä»¶",
            "## ğŸš€ æ–°äº§å“",
            "## ğŸ“Š å¸‚åœºæ–°ç°è±¡",
            "## â“ æœªåˆ†ç±»"
        ]
        
        last_index = -1
        for section in sections:
            current_index = report.find(section)
            if current_index != -1:  # éƒ¨åˆ†å¯èƒ½ä¸å­˜åœ¨ï¼ˆå¦‚ç©ºåˆ†ç±»è¢«çœç•¥ï¼‰
                assert current_index > last_index, f"æŠ¥å‘Šéƒ¨åˆ†é¡ºåºé”™è¯¯: {section} åº”è¯¥åœ¨ä¹‹å‰éƒ¨åˆ†ä¹‹å"
                last_index = current_index
        
        # éªŒè¯æ€»ç»“éƒ¨åˆ†ï¼ˆå¦‚æœæœ‰å†…å®¹ï¼‰
        total_items = sum(len(items) for items in analyzed_data.categorized_items.values())
        if total_items > 0:
            assert "## ğŸ“‹ æŠ¥å‘Šæ€»ç»“" in report, "æœ‰å†…å®¹æ—¶åº”è¯¥åŒ…å«æŠ¥å‘Šæ€»ç»“"
    
    @given(
        analyzed_data=valid_analyzed_data(),
        crawl_status=valid_crawl_status()
    )
    @settings(max_examples=30, deadline=None)
    def test_report_metadata_completeness(self, analyzed_data, crawl_status):
        """
        å±æ€§æµ‹è¯•ï¼šæŠ¥å‘Šå…ƒæ•°æ®å®Œæ•´æ€§
        
        éªŒè¯æŠ¥å‘ŠåŒ…å«æ‰€æœ‰å¿…éœ€çš„å…ƒæ•°æ®ä¿¡æ¯
        """
        report = self.generator.generate_report(analyzed_data, crawl_status)
        
        # éªŒè¯æ—¶é—´ä¿¡æ¯
        assert "ç”Ÿæˆæ—¶é—´" in report, "æŠ¥å‘Šåº”è¯¥åŒ…å«ç”Ÿæˆæ—¶é—´"
        assert "æ•°æ®æ—¶é—´çª—å£" in report, "æŠ¥å‘Šåº”è¯¥åŒ…å«æ—¶é—´çª—å£ä¿¡æ¯"
        assert "æ•°æ®æ—¶é—´èŒƒå›´" in report, "æŠ¥å‘Šåº”è¯¥åŒ…å«æ—¶é—´èŒƒå›´ä¿¡æ¯"
        
        # éªŒè¯æ•°æ®æºä¿¡æ¯
        assert "æ•°æ®æºçˆ¬å–çŠ¶æ€" in report, "æŠ¥å‘Šåº”è¯¥åŒ…å«æ•°æ®æºçŠ¶æ€ä¿¡æ¯"
        
        # éªŒè¯å†…å®¹é¡¹å…ƒæ•°æ®
        all_items = []
        for items in analyzed_data.categorized_items.values():
            all_items.extend(items)
        
        for item in all_items:
            # æ¯ä¸ªå†…å®¹é¡¹åº”è¯¥åŒ…å«å®Œæ•´çš„å…ƒæ•°æ®
            assert "**æ¥æº**:" in report, "å†…å®¹é¡¹åº”è¯¥åŒ…å«æ¥æºä¿¡æ¯"
            assert "**æ—¶é—´**:" in report, "å†…å®¹é¡¹åº”è¯¥åŒ…å«æ—¶é—´ä¿¡æ¯"
            assert "**é“¾æ¥**:" in report, "å†…å®¹é¡¹åº”è¯¥åŒ…å«é“¾æ¥ä¿¡æ¯"
            assert "**å†…å®¹æ‘˜è¦**:" in report, "å†…å®¹é¡¹åº”è¯¥åŒ…å«å†…å®¹æ‘˜è¦"
            
            # éªŒè¯æ¥æºç±»å‹æ ‡è¯†
            source_type_upper = item.source_type.upper()
            assert source_type_upper in report, f"æŠ¥å‘Šåº”è¯¥åŒ…å«æ•°æ®æºç±»å‹æ ‡è¯†: {source_type_upper}"
    
    @given(
        analyzed_data=valid_analyzed_data(),
        crawl_status=valid_crawl_status()
    )
    @settings(max_examples=20, deadline=None)
    def test_report_analysis_integration(self, analyzed_data, crawl_status):
        """
        å±æ€§æµ‹è¯•ï¼šæŠ¥å‘Šåˆ†æç»“æœé›†æˆå®Œæ•´æ€§
        
        éªŒè¯åˆ†æç»“æœæ­£ç¡®é›†æˆåˆ°æŠ¥å‘Šä¸­
        """
        report = self.generator.generate_report(analyzed_data, crawl_status)
        
        # éªŒè¯åˆ†æç»“æœä¿¡æ¯
        for content_id, analysis in analyzed_data.analysis_results.items():
            if not analysis.should_ignore:  # åªæ£€æŸ¥æœªè¢«å¿½ç•¥çš„å†…å®¹
                # éªŒè¯ç½®ä¿¡åº¦ä¿¡æ¯
                confidence_pattern = rf"ç½®ä¿¡åº¦:\s*{analysis.confidence:.2f}"
                assert re.search(confidence_pattern, report), (
                    f"æŠ¥å‘Šåº”è¯¥åŒ…å«åˆ†æç½®ä¿¡åº¦: {analysis.confidence:.2f}"
                )
                
                # éªŒè¯åˆ†æç†ç”±
                if analysis.reasoning:
                    reasoning_pattern = rf"åˆ†æç†ç”±:\s*{re.escape(analysis.reasoning)}"
                    assert re.search(reasoning_pattern, report), (
                        f"æŠ¥å‘Šåº”è¯¥åŒ…å«åˆ†æç†ç”±: {analysis.reasoning}"
                    )
                
                # éªŒè¯å…³é”®ä¿¡æ¯ç‚¹
                if analysis.key_points:
                    assert "å…³é”®ä¿¡æ¯:" in report, "æŠ¥å‘Šåº”è¯¥åŒ…å«å…³é”®ä¿¡æ¯æ ‡ç­¾"
                    for point in analysis.key_points:
                        if point.strip():
                            point_pattern = rf"-\s*{re.escape(point)}"
                            assert re.search(point_pattern, report), (
                                f"æŠ¥å‘Šåº”è¯¥åŒ…å«å…³é”®ä¿¡æ¯ç‚¹: {point}"
                            )
    
    @given(
        time_window_hours=st.integers(min_value=1, max_value=168),  # 1å°æ—¶åˆ°1å‘¨
        item_count=st.integers(min_value=0, max_value=50)
    )
    @settings(max_examples=30, deadline=None)
    def test_report_scalability(self, time_window_hours, item_count):
        """
        å±æ€§æµ‹è¯•ï¼šæŠ¥å‘Šç”Ÿæˆçš„å¯æ‰©å±•æ€§
        
        éªŒè¯ä¸åŒè§„æ¨¡çš„æ•°æ®éƒ½èƒ½ç”Ÿæˆæœ‰æ•ˆæŠ¥å‘Š
        """
        # ç”ŸæˆæŒ‡å®šæ•°é‡çš„å†…å®¹é¡¹
        content_items = []
        analysis_results = {}
        
        for i in range(item_count):
            item = ContentItem(
                id=f"test_item_{i}",
                title=f"æµ‹è¯•æ–°é—» {i}",
                content=f"è¿™æ˜¯ç¬¬ {i} æ¡æµ‹è¯•æ–°é—»å†…å®¹",
                url=f"https://example.com/news/{i}",
                publish_time=datetime.now() - timedelta(hours=i % time_window_hours),
                source_name=f"æµ‹è¯•æº {i % 3}",
                source_type=["rss", "x", "rest_api"][i % 3]
            )
            content_items.append(item)
            
            # ç”Ÿæˆåˆ†æç»“æœ
            categories = ["å¤§æˆ·åŠ¨å‘", "åˆ©ç‡äº‹ä»¶", "å®‰å…¨äº‹ä»¶", "æ–°äº§å“", "å¸‚åœºæ–°ç°è±¡", "æœªåˆ†ç±»"]
            analysis_results[item.id] = AnalysisResult(
                content_id=item.id,
                category=categories[i % len(categories)],
                confidence=0.5 + (i % 5) * 0.1,
                reasoning=f"æµ‹è¯•åˆ†æç†ç”± {i}",
                should_ignore=False,
                key_points=[f"å…³é”®ç‚¹ {i}"]
            )
        
        # æŒ‰ç±»åˆ«åˆ†ç»„
        categorized_items = {
            "å¤§æˆ·åŠ¨å‘": [], "åˆ©ç‡äº‹ä»¶": [], "ç¾å›½æ”¿åºœç›‘ç®¡æ”¿ç­–": [],
            "å®‰å…¨äº‹ä»¶": [], "æ–°äº§å“": [], "å¸‚åœºæ–°ç°è±¡": [], "æœªåˆ†ç±»": []
        }
        
        for item in content_items:
            analysis = analysis_results[item.id]
            categorized_items[analysis.category].append(item)
        
        # åˆ›å»ºåˆ†ææ•°æ®
        analyzed_data = create_analyzed_data(
            categorized_items=categorized_items,
            analysis_results=analysis_results,
            time_window_hours=time_window_hours
        )
        
        # åˆ›å»ºçˆ¬å–çŠ¶æ€
        crawl_status = CrawlStatus(
            rss_results=[CrawlResult("æµ‹è¯•RSSæº", "success", item_count // 2, None)],
            x_results=[CrawlResult("æµ‹è¯•Xæº", "success", item_count - item_count // 2, None)],
            total_items=item_count,
            execution_time=datetime.now()
        )
        
        # ç”ŸæˆæŠ¥å‘Š
        report = self.generator.generate_report(analyzed_data, crawl_status)
        
        # éªŒè¯æŠ¥å‘ŠåŸºæœ¬ç»“æ„
        assert report and report.strip(), "å³ä½¿åœ¨ä¸åŒè§„æ¨¡ä¸‹ä¹Ÿåº”è¯¥ç”Ÿæˆæœ‰æ•ˆæŠ¥å‘Š"
        assert "# åŠ å¯†è´§å¸æ–°é—»åˆ†ææŠ¥å‘Š" in report, "æŠ¥å‘Šåº”è¯¥åŒ…å«æ ‡é¢˜"
        assert f"{time_window_hours} å°æ—¶" in report, "æŠ¥å‘Šåº”è¯¥åŒ…å«æ­£ç¡®çš„æ—¶é—´çª—å£"
        
        # éªŒè¯å†…å®¹æ•°é‡ç»Ÿè®¡
        if item_count > 0:
            assert str(item_count) in report, f"æŠ¥å‘Šåº”è¯¥åŒ…å«æ­£ç¡®çš„å†…å®¹æ•°é‡: {item_count}"
        else:
            assert "æš‚æ— ç›¸å…³å†…å®¹" in report, "ç©ºæŠ¥å‘Šåº”è¯¥æ˜¾ç¤ºæ— å†…å®¹æç¤º"


if __name__ == "__main__":
    # è¿è¡Œå±æ€§æµ‹è¯•
    pytest.main([__file__, "-v", "--tb=short"])