"""
RSSçˆ¬å–å™¨å•å…ƒæµ‹è¯•

æµ‹è¯•RSSçˆ¬å–å™¨çš„æ ¸å¿ƒåŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- RSSå†…å®¹è§£æ
- æ—¶é—´çª—å£è¿‡æ»¤
- é”™è¯¯å¤„ç†
- ç½‘ç»œè¯·æ±‚
"""

import pytest
from unittest.mock import Mock, patch, MagicMock
from datetime import datetime, timedelta
import feedparser
import requests
from bs4 import BeautifulSoup

from crypto_news_analyzer.crawlers.rss_crawler import RSSCrawler
from crypto_news_analyzer.models import RSSSource, ContentItem, CrawlResult
from crypto_news_analyzer.utils.errors import CrawlerError, NetworkError


class TestRSSCrawler:
    """RSSçˆ¬å–å™¨æµ‹è¯•ç±»"""
    
    @pytest.fixture
    def crawler(self):
        """åˆ›å»ºRSSçˆ¬å–å™¨å®ä¾‹"""
        return RSSCrawler(time_window_hours=24)
    
    @pytest.fixture
    def sample_rss_source(self):
        """åˆ›å»ºç¤ºä¾‹RSSæº"""
        return RSSSource(
            name="æµ‹è¯•RSSæº",
            url="https://example.com/rss.xml",
            description="æµ‹è¯•ç”¨RSSæº"
        )
    
    @pytest.fixture
    def sample_rss_content(self):
        """åˆ›å»ºç¤ºä¾‹RSSå†…å®¹"""
        return """<?xml version="1.0" encoding="UTF-8"?>
        <rss version="2.0">
            <channel>
                <title>æµ‹è¯•RSS</title>
                <description>æµ‹è¯•RSSæè¿°</description>
                <link>https://example.com</link>
                <item>
                    <title>æµ‹è¯•æ–°é—»æ ‡é¢˜</title>
                    <description>æµ‹è¯•æ–°é—»å†…å®¹æè¿°</description>
                    <link>https://example.com/news/1</link>
                    <pubDate>Mon, 01 Jan 2024 12:00:00 GMT</pubDate>
                </item>
                <item>
                    <title>è¿‡æœŸæ–°é—»æ ‡é¢˜</title>
                    <description>è¿‡æœŸæ–°é—»å†…å®¹</description>
                    <link>https://example.com/news/2</link>
                    <pubDate>Mon, 01 Jan 2020 12:00:00 GMT</pubDate>
                </item>
            </channel>
        </rss>"""
    
    def test_crawler_initialization(self):
        """æµ‹è¯•çˆ¬å–å™¨åˆå§‹åŒ–"""
        crawler = RSSCrawler(time_window_hours=12, timeout=60)
        
        assert crawler.time_window_hours == 12
        assert crawler.timeout == 60
        assert crawler.session is not None
        assert 'User-Agent' in crawler.session.headers
        
        # æ£€æŸ¥æ—¶é—´çª—å£è®¡ç®— - RSSçˆ¬å–å™¨ä½¿ç”¨UTCæ—¶é—´
        from datetime import timezone
        expected_cutoff = datetime.now(timezone.utc).replace(tzinfo=None) - timedelta(hours=12)
        time_diff = abs((crawler.cutoff_time - expected_cutoff).total_seconds())
        assert time_diff < 5  # å…è®¸5ç§’è¯¯å·®
    
    @patch('crypto_news_analyzer.crawlers.rss_crawler.requests.Session.get')
    def test_fetch_rss_content_success(self, mock_get, crawler, sample_rss_content):
        """æµ‹è¯•æˆåŠŸè·å–RSSå†…å®¹"""
        # æ¨¡æ‹ŸæˆåŠŸå“åº”
        mock_response = Mock()
        mock_response.text = sample_rss_content
        mock_response.headers = {'content-type': 'application/rss+xml'}
        mock_response.raise_for_status.return_value = None
        mock_get.return_value = mock_response
        
        content = crawler._fetch_rss_content("https://example.com/rss.xml")
        
        assert content == sample_rss_content
        mock_get.assert_called_once()
    
    @patch('crypto_news_analyzer.crawlers.rss_crawler.requests.Session.get')
    def test_fetch_rss_content_retry_on_failure(self, mock_get, crawler):
        """æµ‹è¯•ç½‘ç»œå¤±è´¥æ—¶çš„é‡è¯•æœºåˆ¶"""
        # æ¨¡æ‹Ÿå‰ä¸¤æ¬¡å¤±è´¥ï¼Œç¬¬ä¸‰æ¬¡æˆåŠŸ
        mock_get.side_effect = [
            requests.exceptions.ConnectionError("è¿æ¥å¤±è´¥"),
            requests.exceptions.Timeout("è¶…æ—¶"),
            Mock(text="success", headers={}, raise_for_status=Mock())
        ]
        
        content = crawler._fetch_rss_content("https://example.com/rss.xml")
        
        assert content == "success"
        assert mock_get.call_count == 3
    
    @patch('crypto_news_analyzer.crawlers.rss_crawler.requests.Session.get')
    def test_fetch_rss_content_max_retries_exceeded(self, mock_get, crawler):
        """æµ‹è¯•è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°"""
        # æ¨¡æ‹Ÿæ‰€æœ‰è¯·æ±‚éƒ½å¤±è´¥
        mock_get.side_effect = requests.exceptions.ConnectionError("è¿æ¥å¤±è´¥")
        
        with pytest.raises(NetworkError):
            crawler._fetch_rss_content("https://example.com/rss.xml")
        
        assert mock_get.call_count == 3  # é»˜è®¤æœ€å¤§é‡è¯•3æ¬¡
    
    def test_parse_rss_entry_complete_data(self, crawler, sample_rss_source):
        """æµ‹è¯•è§£æå®Œæ•´çš„RSSæ¡ç›®"""
        # åˆ›å»ºæ¨¡æ‹Ÿçš„RSSæ¡ç›®
        entry = Mock()
        entry.title = "æµ‹è¯•æ ‡é¢˜"
        entry.summary = "æµ‹è¯•å†…å®¹æ‘˜è¦"
        entry.link = "https://example.com/news/1"
        entry.published_parsed = (2024, 1, 1, 12, 0, 0, 0, 1, 0)
        
        # æ·»åŠ ç¼ºå¤±çš„å±æ€§
        entry.content = []
        entry.description = ""
        entry.subtitle = ""
        entry.id = ""
        entry.guid = ""
        entry.updated_parsed = None
        entry.created_parsed = None
        entry.published = ""
        entry.updated = ""
        entry.created = ""
        
        item = crawler._parse_rss_entry(entry, sample_rss_source)
        
        assert item is not None
        assert item.title == "æµ‹è¯•æ ‡é¢˜"
        assert item.content == "æµ‹è¯•å†…å®¹æ‘˜è¦"
        assert item.url == "https://example.com/news/1"
        assert item.source_name == sample_rss_source.name
        assert item.source_type == "rss"
        assert isinstance(item.publish_time, datetime)
    
    def test_parse_rss_entry_missing_title(self, crawler, sample_rss_source):
        """æµ‹è¯•ç¼ºå°‘æ ‡é¢˜çš„RSSæ¡ç›®"""
        entry = Mock()
        entry.title = ""
        entry.summary = "æµ‹è¯•å†…å®¹"
        entry.link = "https://example.com/news/1"
        entry.published_parsed = (2024, 1, 1, 12, 0, 0, 0, 1, 0)
        
        # æ·»åŠ ç¼ºå¤±çš„å±æ€§
        entry.content = []
        entry.description = ""
        entry.subtitle = ""
        entry.id = ""
        entry.guid = ""
        
        item = crawler._parse_rss_entry(entry, sample_rss_source)
        
        assert item is None
    
    def test_parse_rss_entry_missing_content(self, crawler, sample_rss_source):
        """æµ‹è¯•ç¼ºå°‘å†…å®¹çš„RSSæ¡ç›® - åº”è¯¥ä½¿ç”¨æ ‡é¢˜ä½œä¸ºå†…å®¹"""
        entry = Mock()
        entry.title = "æµ‹è¯•æ ‡é¢˜"
        entry.summary = ""
        entry.link = "https://example.com/news/1"
        entry.published_parsed = (2024, 1, 1, 12, 0, 0, 0, 1, 0)
        
        # æ·»åŠ ç¼ºå¤±çš„å±æ€§
        entry.content = []
        entry.description = ""
        entry.subtitle = ""
        entry.id = ""
        entry.guid = ""
        
        item = crawler._parse_rss_entry(entry, sample_rss_source)
        
        # å½“å†…å®¹ç¼ºå¤±æ—¶ï¼Œåº”è¯¥ä½¿ç”¨æ ‡é¢˜ä½œä¸ºå†…å®¹ï¼ˆæ”¯æŒåƒCoinDeskè¿™æ ·åªæœ‰æ ‡é¢˜çš„RSSæºï¼‰
        assert item is not None
        assert item.title == "æµ‹è¯•æ ‡é¢˜"
        assert item.content == "æµ‹è¯•æ ‡é¢˜"  # å†…å®¹åº”è¯¥ç­‰äºæ ‡é¢˜
    
    def test_parse_rss_entry_missing_url(self, crawler, sample_rss_source):
        """æµ‹è¯•ç¼ºå°‘URLçš„RSSæ¡ç›®"""
        entry = Mock()
        entry.title = "æµ‹è¯•æ ‡é¢˜"
        entry.summary = "æµ‹è¯•å†…å®¹"
        entry.link = ""
        entry.published_parsed = (2024, 1, 1, 12, 0, 0, 0, 1, 0)
        
        # æ·»åŠ ç¼ºå¤±çš„å±æ€§
        entry.content = []
        entry.description = ""
        entry.subtitle = ""
        entry.id = ""
        entry.guid = ""
        
        item = crawler._parse_rss_entry(entry, sample_rss_source)
        
        assert item is None
    
    def test_extract_content_from_multiple_fields(self, crawler):
        """æµ‹è¯•ä»å¤šä¸ªå­—æ®µæå–å†…å®¹"""
        # æµ‹è¯•contentå­—æ®µï¼ˆåˆ—è¡¨æ ¼å¼ï¼‰
        entry1 = Mock()
        entry1.content = [{'value': 'æ¥è‡ªcontentå­—æ®µçš„å†…å®¹'}]
        entry1.summary = 'æ¥è‡ªsummaryå­—æ®µçš„å†…å®¹'
        
        content1 = crawler._extract_content(entry1)
        assert content1 == 'æ¥è‡ªcontentå­—æ®µçš„å†…å®¹'
        
        # æµ‹è¯•summaryå­—æ®µ
        entry2 = Mock()
        entry2.content = []
        entry2.summary = 'æ¥è‡ªsummaryå­—æ®µçš„å†…å®¹'
        entry2.description = 'æ¥è‡ªdescriptionå­—æ®µçš„å†…å®¹'
        
        content2 = crawler._extract_content(entry2)
        assert content2 == 'æ¥è‡ªsummaryå­—æ®µçš„å†…å®¹'
    
    def test_extract_publish_time_from_multiple_formats(self, crawler):
        """æµ‹è¯•ä»å¤šç§æ ¼å¼æå–å‘å¸ƒæ—¶é—´"""
        # æµ‹è¯•parsedæ ¼å¼
        entry1 = Mock()
        entry1.published_parsed = (2024, 1, 1, 12, 0, 0, 0, 1, 0)
        
        time1 = crawler._extract_publish_time(entry1)
        assert time1 == datetime(2024, 1, 1, 12, 0, 0)
        
        # æµ‹è¯•å­—ç¬¦ä¸²æ ¼å¼
        entry2 = Mock()
        entry2.published_parsed = None
        entry2.published = "2024-01-01T12:00:00Z"
        
        time2 = crawler._extract_publish_time(entry2)
        assert time2.year == 2024
        assert time2.month == 1
        assert time2.day == 1
    
    def test_clean_html(self, crawler):
        """æµ‹è¯•HTMLæ¸…ç†åŠŸèƒ½"""
        html_content = """
        <div>
            <h1>æ ‡é¢˜</h1>
            <p>è¿™æ˜¯ä¸€ä¸ª<strong>é‡è¦</strong>çš„æ®µè½ã€‚</p>
            <script>alert('æ¶æ„è„šæœ¬');</script>
            <style>body { color: red; }</style>
        </div>
        """
        
        clean_content = crawler._clean_html(html_content)
        
        assert "æ ‡é¢˜" in clean_content
        assert "é‡è¦" in clean_content
        assert "æ®µè½" in clean_content
        assert "alert" not in clean_content
        assert "color: red" not in clean_content
        assert "<" not in clean_content
        assert ">" not in clean_content
    
    def test_is_within_time_window(self, crawler):
        """æµ‹è¯•æ—¶é—´çª—å£è¿‡æ»¤"""
        # RSSçˆ¬å–å™¨ä½¿ç”¨UTCæ—¶é—´ï¼Œæ‰€ä»¥æµ‹è¯•ä¹Ÿéœ€è¦ä½¿ç”¨UTC
        from datetime import timezone
        
        # å½“å‰æ—¶é—´ï¼ˆåº”è¯¥åœ¨çª—å£å†…ï¼‰
        current_time = datetime.now(timezone.utc).replace(tzinfo=None)
        assert crawler._is_within_time_window(current_time) is True
        
        # 1å°æ—¶å‰ï¼ˆåº”è¯¥åœ¨çª—å£å†…ï¼‰
        one_hour_ago = datetime.now(timezone.utc).replace(tzinfo=None) - timedelta(hours=1)
        assert crawler._is_within_time_window(one_hour_ago) is True
        
        # 25å°æ—¶å‰ï¼ˆåº”è¯¥åœ¨çª—å£å¤–ï¼Œå› ä¸ºçª—å£æ˜¯24å°æ—¶ï¼‰
        old_time = datetime.now(timezone.utc).replace(tzinfo=None) - timedelta(hours=25)
        assert crawler._is_within_time_window(old_time) is False
    
    def test_is_valid_url(self, crawler):
        """æµ‹è¯•URLéªŒè¯"""
        # æœ‰æ•ˆURL
        assert crawler._is_valid_url("https://example.com") is True
        assert crawler._is_valid_url("http://example.com/path") is True
        assert crawler._is_valid_url("https://subdomain.example.com/path?param=value") is True
        
        # æ— æ•ˆURL
        assert crawler._is_valid_url("") is False
        assert crawler._is_valid_url("not-a-url") is False
        assert crawler._is_valid_url("ftp://example.com") is True  # FTPä¹Ÿæ˜¯æœ‰æ•ˆçš„
        assert crawler._is_valid_url("example.com") is False  # ç¼ºå°‘åè®®
    
    @patch('crypto_news_analyzer.crawlers.rss_crawler.RSSCrawler._fetch_rss_content')
    def test_crawl_source_success(self, mock_fetch, crawler, sample_rss_source, sample_rss_content):
        """æµ‹è¯•æˆåŠŸçˆ¬å–RSSæº"""
        mock_fetch.return_value = sample_rss_content
        
        items = crawler.crawl_source(sample_rss_source)
        
        # åº”è¯¥åªè¿”å›æ—¶é—´çª—å£å†…çš„æ¡ç›®
        assert len(items) >= 0  # å–å†³äºæµ‹è¯•è¿è¡Œæ—¶é—´
        mock_fetch.assert_called_once_with(sample_rss_source.url)
    
    @patch('crypto_news_analyzer.crawlers.rss_crawler.RSSCrawler._fetch_rss_content')
    def test_crawl_source_network_error(self, mock_fetch, crawler, sample_rss_source):
        """æµ‹è¯•ç½‘ç»œé”™è¯¯å¤„ç†"""
        mock_fetch.side_effect = NetworkError("ç½‘ç»œè¿æ¥å¤±è´¥")
        
        with pytest.raises(CrawlerError):
            crawler.crawl_source(sample_rss_source)
    
    @patch('crypto_news_analyzer.crawlers.rss_crawler.RSSCrawler.crawl_source')
    def test_crawl_all_sources_mixed_results(self, mock_crawl_source, crawler):
        """æµ‹è¯•çˆ¬å–å¤šä¸ªæºçš„æ··åˆç»“æœ"""
        # åˆ›å»ºæµ‹è¯•æº
        sources = [
            RSSSource("æº1", "https://example1.com/rss", "æè¿°1"),
            RSSSource("æº2", "https://example2.com/rss", "æè¿°2"),
            RSSSource("æº3", "https://example3.com/rss", "æè¿°3")
        ]
        
        # æ¨¡æ‹Ÿæ··åˆç»“æœï¼šæˆåŠŸã€å¤±è´¥ã€æˆåŠŸ
        mock_items = [
            ContentItem(
                id="test1",
                title="æµ‹è¯•1",
                content="å†…å®¹1",
                url="https://example.com/1",
                publish_time=datetime.now(),
                source_name="æº1",
                source_type="rss"
            )
        ]
        
        mock_crawl_source.side_effect = [
            mock_items,  # æº1æˆåŠŸ
            CrawlerError("æº2å¤±è´¥"),  # æº2å¤±è´¥
            []  # æº3æˆåŠŸä½†æ— å†…å®¹
        ]
        
        result = crawler.crawl_all_sources(sources)
        
        assert len(result['results']) == 3
        assert result['results'][0].status == "success"
        assert result['results'][0].item_count == 1
        assert result['results'][1].status == "error"
        assert result['results'][1].error_message == "æº2å¤±è´¥"
        assert result['results'][2].status == "success"
        assert result['results'][2].item_count == 0
        
        assert result['total_items'] == 1
        assert len(result['items']) == 1
    
    @patch('crypto_news_analyzer.crawlers.rss_crawler.requests.Session.head')
    def test_validate_rss_source(self, mock_head, crawler, sample_rss_source):
        """æµ‹è¯•RSSæºéªŒè¯"""
        # æµ‹è¯•æˆåŠŸéªŒè¯
        mock_head.return_value.status_code = 200
        assert crawler.validate_rss_source(sample_rss_source) is True
        
        # æµ‹è¯•å¤±è´¥éªŒè¯
        mock_head.return_value.status_code = 404
        assert crawler.validate_rss_source(sample_rss_source) is False
        
        # æµ‹è¯•ç½‘ç»œå¼‚å¸¸
        mock_head.side_effect = requests.exceptions.ConnectionError()
        assert crawler.validate_rss_source(sample_rss_source) is False
    
    @patch('crypto_news_analyzer.crawlers.rss_crawler.RSSCrawler._fetch_rss_content')
    def test_get_feed_info(self, mock_fetch, crawler, sample_rss_source, sample_rss_content):
        """æµ‹è¯•è·å–RSSæºä¿¡æ¯"""
        mock_fetch.return_value = sample_rss_content
        
        info = crawler.get_feed_info(sample_rss_source)
        
        assert 'title' in info
        assert 'description' in info
        assert 'entry_count' in info
        assert info['entry_count'] == 2  # sample_rss_contentä¸­æœ‰2ä¸ªæ¡ç›®
    
    def test_extract_url_from_multiple_fields(self, crawler):
        """æµ‹è¯•ä»å¤šä¸ªå­—æ®µæå–URL"""
        # æµ‹è¯•linkå­—æ®µ
        entry1 = Mock()
        entry1.link = "https://example.com/news/1"
        entry1.id = "invalid-id"
        entry1.guid = "invalid-guid"
        
        url1 = crawler._extract_url(entry1)
        assert url1 == "https://example.com/news/1"
        
        # æµ‹è¯•idå­—æ®µ
        entry2 = Mock()
        entry2.link = ""
        entry2.id = "https://example.com/news/2"
        entry2.guid = "invalid-guid"
        
        url2 = crawler._extract_url(entry2)
        assert url2 == "https://example.com/news/2"
        
        # æµ‹è¯•guidå­—æ®µ
        entry3 = Mock()
        entry3.link = ""
        entry3.id = ""
        entry3.guid = "https://example.com/news/3"
        
        url3 = crawler._extract_url(entry3)
        assert url3 == "https://example.com/news/3"


class TestRSSFormats:
    """æµ‹è¯•å„ç§RSSæ ¼å¼çš„è§£æ - éœ€æ±‚ 3.6"""
    
    @pytest.fixture
    def crawler(self):
        """åˆ›å»ºRSSçˆ¬å–å™¨å®ä¾‹"""
        return RSSCrawler(time_window_hours=24)
    
    @pytest.fixture
    def sample_rss_source(self):
        """åˆ›å»ºç¤ºä¾‹RSSæº"""
        return RSSSource(
            name="æµ‹è¯•RSSæº",
            url="https://example.com/rss.xml",
            description="æµ‹è¯•ç”¨RSSæº"
        )
    
    def test_rss_2_0_format_parsing(self, crawler, sample_rss_source):
        """æµ‹è¯•RSS 2.0æ ¼å¼è§£æ"""
        rss_2_0_content = """<?xml version="1.0" encoding="UTF-8"?>
        <rss version="2.0">
            <channel>
                <title>RSS 2.0 æµ‹è¯•</title>
                <description>RSS 2.0 æ ¼å¼æµ‹è¯•</description>
                <link>https://example.com</link>
                
                <item>
                    <title>RSS 2.0 æ–°é—»æ ‡é¢˜</title>
                    <description>RSS 2.0 æ–°é—»å†…å®¹æè¿°</description>
                    <link>https://example.com/news/rss20</link>
                    <pubDate>Mon, 01 Jan 2024 12:00:00 GMT</pubDate>
                    <guid>https://example.com/news/rss20</guid>
                </item>
            </channel>
        </rss>"""
        
        feed = feedparser.parse(rss_2_0_content)
        assert feed.version == "rss20"
        assert len(feed.entries) == 1
        
        item = crawler._parse_rss_entry(feed.entries[0], sample_rss_source)
        assert item is not None
        assert item.title == "RSS 2.0 æ–°é—»æ ‡é¢˜"
        assert item.content == "RSS 2.0 æ–°é—»å†…å®¹æè¿°"
        assert item.url == "https://example.com/news/rss20"
    
    def test_atom_format_parsing(self, crawler, sample_rss_source):
        """æµ‹è¯•Atomæ ¼å¼è§£æ"""
        atom_content = """<?xml version="1.0" encoding="UTF-8"?>
        <feed xmlns="http://www.w3.org/2005/Atom">
            <title>Atom æµ‹è¯•</title>
            <subtitle>Atom æ ¼å¼æµ‹è¯•</subtitle>
            <link href="https://example.com"/>
            <id>https://example.com/atom</id>
            <updated>2024-01-01T12:00:00Z</updated>
            
            <entry>
                <title>Atom æ–°é—»æ ‡é¢˜</title>
                <summary>Atom æ–°é—»å†…å®¹æ‘˜è¦</summary>
                <link href="https://example.com/news/atom"/>
                <id>https://example.com/news/atom</id>
                <updated>2024-01-01T12:00:00Z</updated>
                <published>2024-01-01T12:00:00Z</published>
            </entry>
        </feed>"""
        
        feed = feedparser.parse(atom_content)
        assert feed.version == "atom10"
        assert len(feed.entries) == 1
        
        item = crawler._parse_rss_entry(feed.entries[0], sample_rss_source)
        assert item is not None
        assert item.title == "Atom æ–°é—»æ ‡é¢˜"
        assert item.content == "Atom æ–°é—»å†…å®¹æ‘˜è¦"
        assert item.url == "https://example.com/news/atom"
    
    def test_rss_1_0_format_parsing(self, crawler, sample_rss_source):
        """æµ‹è¯•RSS 1.0 (RDF)æ ¼å¼è§£æ"""
        rss_1_0_content = """<?xml version="1.0" encoding="UTF-8"?>
        <rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
                 xmlns="http://purl.org/rss/1.0/">
            <channel rdf:about="https://example.com">
                <title>RSS 1.0 æµ‹è¯•</title>
                <description>RSS 1.0 æ ¼å¼æµ‹è¯•</description>
                <link>https://example.com</link>
                <items>
                    <rdf:Seq>
                        <rdf:li rdf:resource="https://example.com/news/rss10"/>
                    </rdf:Seq>
                </items>
            </channel>
            
            <item rdf:about="https://example.com/news/rss10">
                <title>RSS 1.0 æ–°é—»æ ‡é¢˜</title>
                <description>RSS 1.0 æ–°é—»å†…å®¹æè¿°</description>
                <link>https://example.com/news/rss10</link>
            </item>
        </rdf:RDF>"""
        
        feed = feedparser.parse(rss_1_0_content)
        assert feed.version == "rss10"
        assert len(feed.entries) == 1
        
        item = crawler._parse_rss_entry(feed.entries[0], sample_rss_source)
        assert item is not None
        assert item.title == "RSS 1.0 æ–°é—»æ ‡é¢˜"
        assert item.content == "RSS 1.0 æ–°é—»å†…å®¹æè¿°"
        assert item.url == "https://example.com/news/rss10"
    
    def test_malformed_rss_parsing(self, crawler, sample_rss_source):
        """æµ‹è¯•æ ¼å¼é”™è¯¯çš„RSSè§£æ"""
        malformed_rss = """<?xml version="1.0" encoding="UTF-8"?>
        <rss version="2.0">
            <channel>
                <title>æ ¼å¼é”™è¯¯çš„RSS</title>
                <description>æµ‹è¯•æ ¼å¼é”™è¯¯çš„RSS</description>
                <!-- ç¼ºå°‘ç»“æŸæ ‡ç­¾çš„item -->
                <item>
                    <title>ä¸å®Œæ•´çš„æ¡ç›®</title>
                    <description>è¿™ä¸ªæ¡ç›®ç¼ºå°‘ç»“æŸæ ‡ç­¾
                
                <item>
                    <title>æ­£å¸¸çš„æ¡ç›®</title>
                    <description>è¿™ä¸ªæ¡ç›®æ˜¯æ­£å¸¸çš„</description>
                    <link>https://example.com/normal</link>
                    <pubDate>Mon, 01 Jan 2024 12:00:00 GMT</pubDate>
                </item>
            </channel>
        </rss>"""
        
        # feedparseråº”è¯¥èƒ½å¤Ÿå¤„ç†æ ¼å¼é”™è¯¯çš„RSS
        feed = feedparser.parse(malformed_rss)
        assert feed.bozo == True  # è¡¨ç¤ºè§£ææ—¶é‡åˆ°äº†é—®é¢˜
        # ä½†ä»ç„¶åº”è¯¥èƒ½è§£æå‡ºä¸€äº›å†…å®¹
        assert len(feed.entries) >= 1
    
    def test_rss_with_cdata_sections(self, crawler, sample_rss_source):
        """æµ‹è¯•åŒ…å«CDATAçš„RSSè§£æ"""
        cdata_rss = """<?xml version="1.0" encoding="UTF-8"?>
        <rss version="2.0">
            <channel>
                <title><![CDATA[åŒ…å«CDATAçš„RSS]]></title>
                <description><![CDATA[æµ‹è¯•CDATAå¤„ç†]]></description>
                
                <item>
                    <title><![CDATA[CDATAæ ‡é¢˜ & ç‰¹æ®Šå­—ç¬¦ < > "]]></title>
                    <description><![CDATA[
                        <p>è¿™æ˜¯åŒ…å«HTMLçš„å†…å®¹</p>
                        <p>ç‰¹æ®Šå­—ç¬¦: & < > " '</p>
                        <script>alert('test');</script>
                    ]]></description>
                    <link>https://example.com/cdata</link>
                    <pubDate>Mon, 01 Jan 2024 12:00:00 GMT</pubDate>
                </item>
            </channel>
        </rss>"""
        
        feed = feedparser.parse(cdata_rss)
        assert len(feed.entries) == 1
        
        item = crawler._parse_rss_entry(feed.entries[0], sample_rss_source)
        assert item is not None
        assert "CDATAæ ‡é¢˜" in item.title
        assert "ç‰¹æ®Šå­—ç¬¦" in item.content
        # ç¡®ä¿HTMLè¢«æ¸…ç†
        assert "<script>" not in item.content
        assert "alert" not in item.content
    
    def test_rss_with_namespaces(self, crawler, sample_rss_source):
        """æµ‹è¯•åŒ…å«å‘½åç©ºé—´çš„RSSè§£æ"""
        namespaced_rss = """<?xml version="1.0" encoding="UTF-8"?>
        <rss version="2.0" 
             xmlns:content="http://purl.org/rss/1.0/modules/content/"
             xmlns:dc="http://purl.org/dc/elements/1.1/">
            <channel>
                <title>å‘½åç©ºé—´RSSæµ‹è¯•</title>
                <description>æµ‹è¯•å‘½åç©ºé—´å¤„ç†</description>
                
                <item>
                    <title>å‘½åç©ºé—´æ–°é—»</title>
                    <description>ç®€çŸ­æè¿°</description>
                    <content:encoded><![CDATA[
                        <p>è¿™æ˜¯å®Œæ•´çš„HTMLå†…å®¹</p>
                        <p>åŒ…å«æ›´å¤šè¯¦ç»†ä¿¡æ¯</p>
                    ]]></content:encoded>
                    <dc:creator>ä½œè€…åç§°</dc:creator>
                    <link>https://example.com/namespace</link>
                    <pubDate>Mon, 01 Jan 2024 12:00:00 GMT</pubDate>
                </item>
            </channel>
        </rss>"""
        
        feed = feedparser.parse(namespaced_rss)
        assert len(feed.entries) == 1
        
        entry = feed.entries[0]
        # feedparseråº”è¯¥èƒ½å¤Ÿå¤„ç†å‘½åç©ºé—´
        assert hasattr(entry, 'content')
        assert hasattr(entry, 'author')
        
        item = crawler._parse_rss_entry(entry, sample_rss_source)
        assert item is not None
        assert item.title == "å‘½åç©ºé—´æ–°é—»"


class TestNetworkErrorHandling:
    """æµ‹è¯•ç½‘ç»œé”™è¯¯å’Œå¼‚å¸¸æƒ…å†µ - éœ€æ±‚ 3.3"""
    
    @pytest.fixture
    def crawler(self):
        """åˆ›å»ºRSSçˆ¬å–å™¨å®ä¾‹"""
        return RSSCrawler(time_window_hours=24)
    
    @pytest.fixture
    def sample_rss_source(self):
        """åˆ›å»ºç¤ºä¾‹RSSæº"""
        return RSSSource(
            name="æµ‹è¯•RSSæº",
            url="https://example.com/rss.xml",
            description="æµ‹è¯•ç”¨RSSæº"
        )
    
    @patch('crypto_news_analyzer.crawlers.rss_crawler.requests.Session.get')
    def test_connection_timeout_error(self, mock_get, crawler, sample_rss_source):
        """æµ‹è¯•è¿æ¥è¶…æ—¶é”™è¯¯"""
        mock_get.side_effect = requests.exceptions.Timeout("è¿æ¥è¶…æ—¶")
        
        with pytest.raises(CrawlerError) as exc_info:
            crawler.crawl_source(sample_rss_source)
        
        assert "è¿æ¥è¶…æ—¶" in str(exc_info.value)
        assert mock_get.call_count == 3  # åº”è¯¥é‡è¯•3æ¬¡
    
    @patch('crypto_news_analyzer.crawlers.rss_crawler.requests.Session.get')
    def test_connection_error(self, mock_get, crawler, sample_rss_source):
        """æµ‹è¯•è¿æ¥é”™è¯¯"""
        mock_get.side_effect = requests.exceptions.ConnectionError("æ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨")
        
        with pytest.raises(CrawlerError) as exc_info:
            crawler.crawl_source(sample_rss_source)
        
        assert "æ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨" in str(exc_info.value)
        assert mock_get.call_count == 3  # åº”è¯¥é‡è¯•3æ¬¡
    
    @patch('crypto_news_analyzer.crawlers.rss_crawler.requests.Session.get')
    def test_http_error_404(self, mock_get, crawler, sample_rss_source):
        """æµ‹è¯•HTTP 404é”™è¯¯"""
        mock_response = Mock()
        mock_response.raise_for_status.side_effect = requests.exceptions.HTTPError("404 Not Found")
        mock_get.return_value = mock_response
        
        with pytest.raises(CrawlerError) as exc_info:
            crawler.crawl_source(sample_rss_source)
        
        assert "404 Not Found" in str(exc_info.value)
        assert mock_get.call_count == 3  # åº”è¯¥é‡è¯•3æ¬¡
    
    @patch('crypto_news_analyzer.crawlers.rss_crawler.requests.Session.get')
    def test_http_error_500(self, mock_get, crawler, sample_rss_source):
        """æµ‹è¯•HTTP 500é”™è¯¯"""
        mock_response = Mock()
        mock_response.raise_for_status.side_effect = requests.exceptions.HTTPError("500 Internal Server Error")
        mock_get.return_value = mock_response
        
        with pytest.raises(CrawlerError) as exc_info:
            crawler.crawl_source(sample_rss_source)
        
        assert "500 Internal Server Error" in str(exc_info.value)
    
    @patch('crypto_news_analyzer.crawlers.rss_crawler.requests.Session.get')
    def test_dns_resolution_error(self, mock_get, crawler, sample_rss_source):
        """æµ‹è¯•DNSè§£æé”™è¯¯"""
        mock_get.side_effect = requests.exceptions.ConnectionError("DNSè§£æå¤±è´¥")
        
        with pytest.raises(CrawlerError):
            crawler.crawl_source(sample_rss_source)
    
    @patch('crypto_news_analyzer.crawlers.rss_crawler.requests.Session.get')
    def test_ssl_error(self, mock_get, crawler, sample_rss_source):
        """æµ‹è¯•SSLè¯ä¹¦é”™è¯¯"""
        mock_get.side_effect = requests.exceptions.SSLError("SSLè¯ä¹¦éªŒè¯å¤±è´¥")
        
        with pytest.raises(CrawlerError):
            crawler.crawl_source(sample_rss_source)
    
    @patch('crypto_news_analyzer.crawlers.rss_crawler.requests.Session.get')
    def test_invalid_response_content(self, mock_get, crawler, sample_rss_source):
        """æµ‹è¯•æ— æ•ˆå“åº”å†…å®¹"""
        mock_response = Mock()
        mock_response.text = "è¿™ä¸æ˜¯æœ‰æ•ˆçš„XMLå†…å®¹"
        mock_response.headers = {'content-type': 'text/html'}
        mock_response.raise_for_status.return_value = None
        mock_get.return_value = mock_response
        
        # åº”è¯¥èƒ½å¤Ÿå¤„ç†æ— æ•ˆå†…å®¹è€Œä¸å´©æºƒ
        items = crawler.crawl_source(sample_rss_source)
        assert items == []  # æ— æ•ˆå†…å®¹åº”è¯¥è¿”å›ç©ºåˆ—è¡¨
    
    @patch('crypto_news_analyzer.crawlers.rss_crawler.requests.Session.get')
    def test_empty_response(self, mock_get, crawler, sample_rss_source):
        """æµ‹è¯•ç©ºå“åº”"""
        mock_response = Mock()
        mock_response.text = ""
        mock_response.headers = {'content-type': 'application/rss+xml'}
        mock_response.raise_for_status.return_value = None
        mock_get.return_value = mock_response
        
        items = crawler.crawl_source(sample_rss_source)
        assert items == []
    
    @patch('crypto_news_analyzer.crawlers.rss_crawler.requests.Session.get')
    def test_partial_network_failure_in_batch(self, mock_get, crawler):
        """æµ‹è¯•æ‰¹é‡çˆ¬å–ä¸­çš„éƒ¨åˆ†ç½‘ç»œå¤±è´¥ - éœ€æ±‚ 3.3"""
        sources = [
            RSSSource("æˆåŠŸæº1", "https://success1.com/rss", "æˆåŠŸçš„æº1"),
            RSSSource("å¤±è´¥æº", "https://fail.com/rss", "å¤±è´¥çš„æº"),
            RSSSource("æˆåŠŸæº2", "https://success2.com/rss", "æˆåŠŸçš„æº2")
        ]
        
        # æ¨¡æ‹Ÿç¬¬äºŒä¸ªæºå¤±è´¥ï¼Œå…¶ä»–æˆåŠŸ
        success_response = Mock()
        success_response.text = """<?xml version="1.0"?>
        <rss version="2.0">
            <channel>
                <title>æµ‹è¯•</title>
                <item>
                    <title>æµ‹è¯•æ–°é—»</title>
                    <description>æµ‹è¯•å†…å®¹</description>
                    <link>https://example.com/news</link>
                    <pubDate>Mon, 01 Jan 2024 12:00:00 GMT</pubDate>
                </item>
            </channel>
        </rss>"""
        success_response.headers = {'content-type': 'application/rss+xml'}
        success_response.raise_for_status.return_value = None
        
        # åˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥æ§åˆ¶æ¯æ¬¡è°ƒç”¨çš„è¿”å›å€¼
        call_count = 0
        def mock_get_side_effect(*args, **kwargs):
            nonlocal call_count
            call_count += 1
            if call_count == 1:
                return success_response  # ç¬¬ä¸€ä¸ªæºæˆåŠŸ
            elif call_count <= 4:  # ç¬¬äºŒä¸ªæºå¤±è´¥ï¼ˆåŒ…æ‹¬é‡è¯•ï¼‰
                raise requests.exceptions.ConnectionError("è¿æ¥å¤±è´¥")
            else:
                return success_response  # ç¬¬ä¸‰ä¸ªæºæˆåŠŸ
        
        mock_get.side_effect = mock_get_side_effect
        
        result = crawler.crawl_all_sources(sources)
        
        # éªŒè¯ç»“æœ
        assert len(result['results']) == 3
        assert result['results'][0].status == "success"
        assert result['results'][1].status == "error"
        assert result['results'][2].status == "success"
        assert "è¿æ¥å¤±è´¥" in result['results'][1].error_message
        
        # åº”è¯¥æœ‰æ¥è‡ªæˆåŠŸæºçš„å†…å®¹
        assert result['total_items'] >= 0  # å¯èƒ½å› ä¸ºæ—¶é—´çª—å£è¿‡æ»¤è€Œä¸º0
    
    @patch('crypto_news_analyzer.crawlers.rss_crawler.requests.Session.get')
    def test_encoding_error_handling(self, mock_get, crawler, sample_rss_source):
        """æµ‹è¯•ç¼–ç é”™è¯¯å¤„ç†"""
        # æ¨¡æ‹ŸåŒ…å«ç‰¹æ®Šç¼–ç çš„å“åº”
        mock_response = Mock()
        mock_response.text = """<?xml version="1.0" encoding="UTF-8"?>
        <rss version="2.0">
            <channel>
                <title>ç¼–ç æµ‹è¯•</title>
                <item>
                    <title>åŒ…å«ç‰¹æ®Šå­—ç¬¦çš„æ ‡é¢˜ Ã±Ã¡Ã©Ã­Ã³Ãº</title>
                    <description>åŒ…å«emojiçš„å†…å®¹ ğŸš€ ğŸ’° ğŸ“ˆ</description>
                    <link>https://example.com/encoding</link>
                    <pubDate>Mon, 01 Jan 2024 12:00:00 GMT</pubDate>
                </item>
            </channel>
        </rss>"""
        mock_response.headers = {'content-type': 'application/rss+xml; charset=utf-8'}
        mock_response.raise_for_status.return_value = None
        mock_get.return_value = mock_response
        
        # åº”è¯¥èƒ½å¤Ÿæ­£ç¡®å¤„ç†ç‰¹æ®Šå­—ç¬¦
        items = crawler.crawl_source(sample_rss_source)
        if items:  # å¦‚æœåœ¨æ—¶é—´çª—å£å†…
            assert "Ã±Ã¡Ã©Ã­Ã³Ãº" in items[0].title
            assert "ğŸš€" in items[0].content


class TestRSSCrawlerIntegration:
    """RSSçˆ¬å–å™¨é›†æˆæµ‹è¯•"""
    
    def test_real_rss_parsing(self):
        """æµ‹è¯•çœŸå®RSSè§£æï¼ˆä½¿ç”¨æœ¬åœ°RSSå†…å®¹ï¼‰"""
        # åˆ›å»ºä¸€ä¸ªçœŸå®çš„RSSå†…å®¹ç”¨äºæµ‹è¯•
        real_rss_content = """<?xml version="1.0" encoding="UTF-8"?>
        <rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
            <channel>
                <title>åŠ å¯†è´§å¸æ–°é—»æµ‹è¯•</title>
                <description>æµ‹è¯•ç”¨çš„åŠ å¯†è´§å¸æ–°é—»RSS</description>
                <link>https://crypto-news-test.com</link>
                <atom:link href="https://crypto-news-test.com/rss" rel="self" type="application/rss+xml"/>
                
                <item>
                    <title><![CDATA[æ¯”ç‰¹å¸ä»·æ ¼çªç ´æ–°é«˜]]></title>
                    <description><![CDATA[
                        <p>æ¯”ç‰¹å¸ä»·æ ¼ä»Šæ—¥çªç ´å†å²æ–°é«˜ï¼Œè¾¾åˆ° $50,000 ç¾å…ƒã€‚</p>
                        <p>å¸‚åœºåˆ†æå¸ˆè®¤ä¸ºè¿™æ˜¯ç”±äºæœºæ„æŠ•èµ„è€…çš„å¤§é‡ä¹°å…¥ã€‚</p>
                    ]]></description>
                    <link>https://crypto-news-test.com/bitcoin-new-high</link>
                    <guid>https://crypto-news-test.com/bitcoin-new-high</guid>
                    <pubDate>Mon, 01 Jan 2024 10:00:00 GMT</pubDate>
                </item>
                
                <item>
                    <title>ä»¥å¤ªåŠå‡çº§å®Œæˆ</title>
                    <description>ä»¥å¤ªåŠç½‘ç»œæˆåŠŸå®Œæˆæœ€æ–°å‡çº§ï¼Œæé«˜äº†äº¤æ˜“æ•ˆç‡ã€‚</description>
                    <link>https://crypto-news-test.com/ethereum-upgrade</link>
                    <pubDate>Sun, 31 Dec 2023 15:30:00 GMT</pubDate>
                </item>
            </channel>
        </rss>"""
        
        # è§£æRSSå†…å®¹
        feed = feedparser.parse(real_rss_content)
        
        # éªŒè¯è§£æç»“æœ
        assert feed.feed.title == "åŠ å¯†è´§å¸æ–°é—»æµ‹è¯•"
        assert len(feed.entries) == 2
        
        # éªŒè¯ç¬¬ä¸€ä¸ªæ¡ç›®
        entry1 = feed.entries[0]
        assert "æ¯”ç‰¹å¸ä»·æ ¼çªç ´æ–°é«˜" in entry1.title
        assert "50,000" in entry1.description
        assert entry1.link == "https://crypto-news-test.com/bitcoin-new-high"
        
        # éªŒè¯ç¬¬äºŒä¸ªæ¡ç›®
        entry2 = feed.entries[1]
        assert "ä»¥å¤ªåŠå‡çº§å®Œæˆ" in entry2.title
        assert "äº¤æ˜“æ•ˆç‡" in entry2.description


if __name__ == "__main__":
    pytest.main([__file__])