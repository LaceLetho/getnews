"""
X/Twitterå†…å®¹è§£æå®Œæ•´æ€§å±æ€§æµ‹è¯•

ä½¿ç”¨Hypothesisè¿›è¡Œå±æ€§æµ‹è¯•ï¼ŒéªŒè¯X/Twitterå†…å®¹è§£æçš„å®Œæ•´æ€§ã€‚
**åŠŸèƒ½: crypto-news-analyzer, å±æ€§ 4: å†…å®¹è§£æå®Œæ•´æ€§**
**éªŒè¯: éœ€æ±‚ 4.5**
"""

import pytest
from datetime import datetime, timedelta
from unittest.mock import Mock
from hypothesis import given, strategies as st, assume, settings
from typing import List, Dict, Any, Optional

from crypto_news_analyzer.crawlers.x_crawler import XCrawler
from crypto_news_analyzer.models import XSource, ContentItem


# ç­–ç•¥å®šä¹‰ï¼šç”Ÿæˆæœ‰æ•ˆçš„X/Twitteræ•°æ®
@st.composite
def valid_x_tweet_data(draw):
    """ç”Ÿæˆæœ‰æ•ˆçš„æ¨æ–‡æ•°æ®"""
    # ç”Ÿæˆç®€å•ä½†æœ‰æ„ä¹‰çš„æ¨æ–‡å†…å®¹
    tweet_texts = [
        "æ¯”ç‰¹å¸çªç ´æ–°é«˜ï¼ğŸš€ #Bitcoin #Crypto",
        "ä»¥å¤ªåŠ2.0å‡çº§å®Œæˆï¼Œç½‘ç»œæ€§èƒ½å¤§å¹…æå‡ $ETH",
        "åŠ å¯†è´§å¸å¸‚åœºä»Šæ—¥è¡¨ç°å¼ºåŠ²ï¼Œä¸»æµå¸ç§æ™®æ¶¨",
        "DeFiåè®®é”ä»“é‡åˆ›å†å²æ–°é«˜ #DeFi",
        "NFTå¸‚åœºå‡ºç°æ–°è¶‹åŠ¿ï¼Œè‰ºæœ¯å“äº¤æ˜“æ´»è·ƒ",
        "å¤®è¡Œæ•°å­—è´§å¸CBDCè¯•ç‚¹æ‰©å¤§èŒƒå›´"
    ]
    
    usernames = [
        "crypto_analyst", "blockchain_news", "defi_tracker",
        "nft_collector", "bitcoin_whale", "eth_developer"
    ]
    
    text = draw(st.sampled_from(tweet_texts))
    username = draw(st.sampled_from(usernames))
    
    # ç”Ÿæˆæ¨æ–‡ID
    tweet_id = draw(st.integers(min_value=1000000000000000000, max_value=9999999999999999999))
    
    # ç”Ÿæˆæ—¶é—´ï¼ˆåœ¨åˆç†èŒƒå›´å†…ï¼‰
    now = datetime.now()
    hours_ago = draw(st.integers(min_value=0, max_value=48))
    publish_time = now - timedelta(hours=hours_ago)
    
    # ç”ŸæˆTwitteræ—¶é—´æ ¼å¼å­—ç¬¦ä¸²
    created_at = publish_time.strftime("%a %b %d %H:%M:%S +0000 %Y")
    
    return {
        "id": str(tweet_id),
        "text": text,
        "created_at": created_at,
        "username": username,
        "publish_time": publish_time
    }


@st.composite
def x_tweet_with_variations(draw):
    """ç”Ÿæˆå…·æœ‰ä¸åŒå­—æ®µå˜ä½“çš„æ¨æ–‡æ•°æ®"""
    base_data = draw(valid_x_tweet_data())
    
    # åˆ›å»ºæ¨¡æ‹Ÿçš„æ¨æ–‡æ•°æ®ç»“æ„
    tweet_data = {
        "id": base_data["id"],
        "text": base_data["text"],
        "created_at": base_data["created_at"],
        "user": {
            "screen_name": base_data["username"],
            "name": f"{base_data['username'].title()} User",
            "id": draw(st.integers(min_value=1000000, max_value=9999999999))
        },
        "entities": {
            "hashtags": [],
            "urls": [],
            "user_mentions": []
        },
        "public_metrics": {
            "retweet_count": draw(st.integers(min_value=0, max_value=1000)),
            "like_count": draw(st.integers(min_value=0, max_value=5000)),
            "reply_count": draw(st.integers(min_value=0, max_value=100))
        }
    }
    
    return tweet_data, base_data


@st.composite
def x_timeline_response_data(draw):
    """ç”Ÿæˆæ¨¡æ‹Ÿçš„Xæ—¶é—´çº¿å“åº”æ•°æ®"""
    tweets_count = draw(st.integers(min_value=1, max_value=5))
    tweets = []
    expected_data = []
    
    for _ in range(tweets_count):
        tweet_data, base_data = draw(x_tweet_with_variations())
        tweets.append(tweet_data)
        expected_data.append(base_data)
    
    # æ„å»ºæ¨¡æ‹Ÿçš„APIå“åº”ç»“æ„
    response_data = {
        "data": {
            "home": {
                "home_timeline_urt": {
                    "instructions": [
                        {
                            "type": "TimelineAddEntries",
                            "entries": []
                        }
                    ]
                }
            }
        }
    }
    
    # ä¸ºæ¯ä¸ªæ¨æ–‡åˆ›å»ºæ¡ç›®
    for i, tweet_data in enumerate(tweets):
        entry = {
            "entryId": f"tweet-{tweet_data['id']}",
            "content": {
                "itemContent": {
                    "tweet_results": {
                        "result": {
                            "rest_id": tweet_data["id"],
                            "legacy": {
                                "full_text": tweet_data["text"],
                                "created_at": tweet_data["created_at"],
                                "entities": tweet_data["entities"],
                                "public_metrics": tweet_data["public_metrics"]
                            },
                            "core": {
                                "user_results": {
                                    "result": {
                                        "legacy": tweet_data["user"]
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }
        response_data["data"]["home"]["home_timeline_urt"]["instructions"][0]["entries"].append(entry)
    
    return response_data, tweets, expected_data


class TestXContentParsingProperties:
    """X/Twitterå†…å®¹è§£æå®Œæ•´æ€§å±æ€§æµ‹è¯•"""
    
    def setup_method(self):
        """æµ‹è¯•å‰è®¾ç½®"""
        # ä½¿ç”¨æ¨¡æ‹Ÿçš„è®¤è¯ä¿¡æ¯
        self.crawler = XCrawler(
            ct0="mock_ct0_token",
            auth_token="mock_auth_token",
            time_window_hours=72  # ä½¿ç”¨æ›´å¤§çš„æ—¶é—´çª—å£
        )
        self.sample_source = XSource(
            name="æµ‹è¯•Xæº",
            url="https://x.com/i/lists/1234567890",
            type="list"
        )
    
    @given(tweet_data=x_tweet_with_variations())
    @settings(max_examples=100, deadline=None)
    def test_x_content_parsing_completeness(self, tweet_data):
        """
        å±æ€§æµ‹è¯•ï¼šXå†…å®¹è§£æå®Œæ•´æ€§
        
        **åŠŸèƒ½: crypto-news-analyzer, å±æ€§ 4: å†…å®¹è§£æå®Œæ•´æ€§**
        **éªŒè¯: éœ€æ±‚ 4.5**
        
        å¯¹äºä»»ä½•æœ‰æ•ˆçš„Xå†…å®¹ï¼Œè§£æåçš„ContentItemåº”è¯¥åŒ…å«æ ‡é¢˜ã€å†…å®¹ã€å‘å¸ƒæ—¶é—´å’ŒåŸæ–‡é“¾æ¥ç­‰æ‰€æœ‰å¿…éœ€å­—æ®µ
        """
        tweet_raw, expected_data = tweet_data
        
        # è§£ææ¨æ–‡æ•°æ®
        result = self.crawler.parse_tweet(tweet_raw)
        
        # éªŒè¯ï¼šè§£ææˆåŠŸæ—¶åº”è¯¥æ˜¯ContentItemå¯¹è±¡
        assert isinstance(result, ContentItem), "è§£æç»“æœåº”è¯¥æ˜¯ContentItemå¯¹è±¡"
        
        # éªŒè¯ï¼šæ‰€æœ‰å¿…éœ€å­—æ®µéƒ½å­˜åœ¨ä¸”éç©º
        assert result.title, "æ ‡é¢˜å­—æ®µä¸èƒ½ä¸ºç©º"
        assert result.content, "å†…å®¹å­—æ®µä¸èƒ½ä¸ºç©º"
        assert result.url, "URLå­—æ®µä¸èƒ½ä¸ºç©º"
        assert result.publish_time, "å‘å¸ƒæ—¶é—´å­—æ®µä¸èƒ½ä¸ºç©º"
        assert result.source_name, "æ•°æ®æºåç§°ä¸èƒ½ä¸ºç©º"
        assert result.source_type, "æ•°æ®æºç±»å‹ä¸èƒ½ä¸ºç©º"
        
        # éªŒè¯ï¼šå­—æ®µç±»å‹æ­£ç¡®
        assert isinstance(result.title, str), "æ ‡é¢˜åº”è¯¥æ˜¯å­—ç¬¦ä¸²"
        assert isinstance(result.content, str), "å†…å®¹åº”è¯¥æ˜¯å­—ç¬¦ä¸²"
        assert isinstance(result.url, str), "URLåº”è¯¥æ˜¯å­—ç¬¦ä¸²"
        assert isinstance(result.publish_time, datetime), "å‘å¸ƒæ—¶é—´åº”è¯¥æ˜¯datetimeå¯¹è±¡"
        assert isinstance(result.source_name, str), "æ•°æ®æºåç§°åº”è¯¥æ˜¯å­—ç¬¦ä¸²"
        assert isinstance(result.source_type, str), "æ•°æ®æºç±»å‹åº”è¯¥æ˜¯å­—ç¬¦ä¸²"
        
        # éªŒè¯ï¼šå­—æ®µå†…å®¹æ­£ç¡®
        assert expected_data["text"] in result.content, "å†…å®¹åº”è¯¥åŒ…å«åŸå§‹æ¨æ–‡æ–‡æœ¬"
        assert expected_data["username"] in result.title, "æ ‡é¢˜åº”è¯¥åŒ…å«ç”¨æˆ·å"
        assert expected_data["id"] in result.url, "URLåº”è¯¥åŒ…å«æ¨æ–‡ID"
        assert result.source_name == "X/Twitter", "æ•°æ®æºåç§°åº”è¯¥æ˜¯X/Twitter"
        assert result.source_type == "x", "æ•°æ®æºç±»å‹åº”è¯¥æ˜¯x"
        
        # éªŒè¯ï¼šURLæ ¼å¼æ­£ç¡®
        expected_url = f"https://x.com/{expected_data['username']}/status/{expected_data['id']}"
        assert result.url == expected_url, f"URLæ ¼å¼ä¸æ­£ç¡®ï¼ŒæœŸæœ›: {expected_url}, å®é™…: {result.url}"
        
        # éªŒè¯ï¼šå‘å¸ƒæ—¶é—´åœ¨åˆç†èŒƒå›´å†…
        time_diff = abs((result.publish_time - expected_data["publish_time"]).total_seconds())
        assert time_diff < 60, f"å‘å¸ƒæ—¶é—´å·®å¼‚è¿‡å¤§: {time_diff}ç§’"
        
        # éªŒè¯ï¼šå†…å®¹å®Œæ•´æ€§
        assert len(result.content.strip()) > 0, "å†…å®¹ä¸èƒ½ä¸ºç©º"
        assert result.content == expected_data["text"], "å†…å®¹åº”è¯¥ä¸åŸå§‹æ¨æ–‡æ–‡æœ¬å®Œå…¨åŒ¹é…"
    
    @given(
        tweets=st.lists(x_tweet_with_variations(), min_size=1, max_size=3),
        time_window=st.integers(min_value=48, max_value=72)
    )
    @settings(max_examples=20, deadline=None)
    def test_batch_parsing_completeness(self, tweets, time_window):
        """
        å±æ€§æµ‹è¯•ï¼šæ‰¹é‡è§£æçš„å®Œæ•´æ€§
        
        éªŒè¯æ‰¹é‡è§£æå¤šä¸ªæ¨æ–‡æ—¶ï¼Œæ¯ä¸ªæœ‰æ•ˆæ¨æ–‡éƒ½èƒ½æ­£ç¡®è§£æ
        """
        crawler = XCrawler(
            ct0="mock_ct0_token",
            auth_token="mock_auth_token", 
            time_window_hours=time_window
        )
        
        valid_results = []
        expected_count = 0
        
        for tweet_raw, expected_data in tweets:
            try:
                result = crawler.parse_tweet(tweet_raw)
                valid_results.append(result)
                
                # åªæœ‰åœ¨æ—¶é—´çª—å£å†…çš„æ¨æ–‡æ‰åº”è¯¥è¢«è®¡å…¥æœŸæœ›æ•°é‡
                if crawler.is_within_time_window(expected_data["publish_time"]):
                    expected_count += 1
                    
            except Exception as e:
                # è®°å½•è§£æå¤±è´¥çš„æƒ…å†µï¼Œä½†ä¸ä¸­æ–­æµ‹è¯•
                pytest.fail(f"æ¨æ–‡è§£æå¤±è´¥: {str(e)}")
        
        # éªŒè¯ï¼šæ‰€æœ‰è§£ææˆåŠŸçš„æ¨æ–‡éƒ½åº”è¯¥åœ¨æ—¶é—´çª—å£å†…
        filtered_results = [
            result for result in valid_results 
            if crawler.is_within_time_window(result.publish_time)
        ]
        
        assert len(filtered_results) == expected_count, \
            f"æ—¶é—´çª—å£å†…è§£æç»“æœæ•°é‡ä¸åŒ¹é…ï¼šæœŸæœ› {expected_count}ï¼Œå®é™… {len(filtered_results)}"
        
        # éªŒè¯ï¼šæ¯ä¸ªè§£æç»“æœéƒ½åŒ…å«å®Œæ•´å­—æ®µ
        for result in filtered_results:
            assert result.title, "æ‰¹é‡è§£æä¸­çš„æ ‡é¢˜å­—æ®µä¸èƒ½ä¸ºç©º"
            assert result.content, "æ‰¹é‡è§£æä¸­çš„å†…å®¹å­—æ®µä¸èƒ½ä¸ºç©º"
            assert result.url, "æ‰¹é‡è§£æä¸­çš„URLå­—æ®µä¸èƒ½ä¸ºç©º"
            assert result.publish_time, "æ‰¹é‡è§£æä¸­çš„å‘å¸ƒæ—¶é—´å­—æ®µä¸èƒ½ä¸ºç©º"
            assert result.source_name == "X/Twitter", "æ‰¹é‡è§£æä¸­çš„æ•°æ®æºåç§°ä¸åŒ¹é…"
            assert result.source_type == "x", "æ‰¹é‡è§£æä¸­çš„æ•°æ®æºç±»å‹ä¸åŒ¹é…"
    
    @given(response_data=x_timeline_response_data())
    @settings(max_examples=30, deadline=None)
    def test_timeline_response_parsing_completeness(self, response_data):
        """
        å±æ€§æµ‹è¯•ï¼šæ—¶é—´çº¿å“åº”è§£æçš„å®Œæ•´æ€§
        
        éªŒè¯ä»X APIå“åº”ä¸­è§£ææ¨æ–‡æ•°æ®çš„å®Œæ•´æ€§
        """
        api_response, tweet_list, expected_list = response_data
        
        # è§£ææ—¶é—´çº¿å“åº”
        parsed_tweets = self.crawler._parse_timeline_response(api_response)
        
        # éªŒè¯ï¼šè§£æç»“æœæ•°é‡æ­£ç¡®
        assert len(parsed_tweets) == len(tweet_list), \
            f"è§£æçš„æ¨æ–‡æ•°é‡ä¸åŒ¹é…ï¼šæœŸæœ› {len(tweet_list)}ï¼Œå®é™… {len(parsed_tweets)}"
        
        # éªŒè¯ï¼šæ¯ä¸ªè§£æçš„æ¨æ–‡éƒ½åŒ…å«å¿…éœ€å­—æ®µ
        for i, parsed_tweet in enumerate(parsed_tweets):
            expected_data = expected_list[i]
            
            assert "id" in parsed_tweet, "è§£æçš„æ¨æ–‡åº”è¯¥åŒ…å«IDå­—æ®µ"
            assert "text" in parsed_tweet, "è§£æçš„æ¨æ–‡åº”è¯¥åŒ…å«æ–‡æœ¬å­—æ®µ"
            assert "created_at" in parsed_tweet, "è§£æçš„æ¨æ–‡åº”è¯¥åŒ…å«åˆ›å»ºæ—¶é—´å­—æ®µ"
            assert "user" in parsed_tweet, "è§£æçš„æ¨æ–‡åº”è¯¥åŒ…å«ç”¨æˆ·å­—æ®µ"
            
            # éªŒè¯å­—æ®µå†…å®¹æ­£ç¡®
            assert parsed_tweet["id"] == expected_data["id"], "æ¨æ–‡IDä¸åŒ¹é…"
            assert parsed_tweet["text"] == expected_data["text"], "æ¨æ–‡æ–‡æœ¬ä¸åŒ¹é…"
            assert parsed_tweet["created_at"] == expected_data["created_at"], "åˆ›å»ºæ—¶é—´ä¸åŒ¹é…"
            assert parsed_tweet["user"]["screen_name"] == expected_data["username"], "ç”¨æˆ·åä¸åŒ¹é…"
    
    @given(tweet_data=x_tweet_with_variations())
    @settings(max_examples=50, deadline=None)
    def test_twitter_time_parsing_robustness(self, tweet_data):
        """
        å±æ€§æµ‹è¯•ï¼šTwitteræ—¶é—´è§£æçš„å¥å£®æ€§
        
        éªŒè¯Twitteræ—¶é—´æ ¼å¼è§£æçš„æ­£ç¡®æ€§
        """
        tweet_raw, expected_data = tweet_data
        
        # æµ‹è¯•æ—¶é—´è§£æ
        parsed_time = self.crawler._parse_twitter_time(expected_data["created_at"])
        
        # éªŒè¯ï¼šè§£æç»“æœæ˜¯datetimeå¯¹è±¡
        assert isinstance(parsed_time, datetime), "è§£æçš„æ—¶é—´åº”è¯¥æ˜¯datetimeå¯¹è±¡"
        
        # éªŒè¯ï¼šæ—¶é—´åœ¨åˆç†èŒƒå›´å†…ï¼ˆå…è®¸ä¸€å®šè¯¯å·®ï¼‰
        time_diff = abs((parsed_time - expected_data["publish_time"]).total_seconds())
        assert time_diff < 60, f"æ—¶é—´è§£æè¯¯å·®è¿‡å¤§: {time_diff}ç§’"
        
        # éªŒè¯ï¼šæ—¶é—´ä¸æ˜¯æœªæ¥æ—¶é—´
        assert parsed_time <= datetime.now(), "è§£æçš„æ—¶é—´ä¸åº”è¯¥æ˜¯æœªæ¥æ—¶é—´"
    
    @given(
        tweets=st.lists(x_tweet_with_variations(), min_size=2, max_size=5)
    )
    @settings(max_examples=15, deadline=None)
    def test_parsing_consistency_across_tweets(self, tweets):
        """
        å±æ€§æµ‹è¯•ï¼šè·¨æ¨æ–‡è§£æçš„ä¸€è‡´æ€§
        
        éªŒè¯è§£æå¤šä¸ªæ¨æ–‡æ—¶çš„ä¸€è‡´æ€§è¡Œä¸º
        """
        results = []
        
        for tweet_raw, expected_data in tweets:
            try:
                result = self.crawler.parse_tweet(tweet_raw)
                results.append((result, expected_data))
            except Exception as e:
                pytest.fail(f"æ¨æ–‡è§£æå¤±è´¥: {str(e)}")
        
        # éªŒè¯ï¼šæ‰€æœ‰è§£æç»“æœéƒ½å…·æœ‰ä¸€è‡´çš„ç»“æ„
        if results:
            first_result, _ = results[0]
            
            for result, expected_data in results:
                # éªŒè¯ï¼šæ‰€æœ‰ç»“æœéƒ½æœ‰ç›¸åŒçš„å­—æ®µç±»å‹
                assert type(result.title) == type(first_result.title), "æ ‡é¢˜ç±»å‹ä¸ä¸€è‡´"
                assert type(result.content) == type(first_result.content), "å†…å®¹ç±»å‹ä¸ä¸€è‡´"
                assert type(result.url) == type(first_result.url), "URLç±»å‹ä¸ä¸€è‡´"
                assert type(result.publish_time) == type(first_result.publish_time), "æ—¶é—´ç±»å‹ä¸ä¸€è‡´"
                assert type(result.source_name) == type(first_result.source_name), "æ•°æ®æºåç§°ç±»å‹ä¸ä¸€è‡´"
                assert type(result.source_type) == type(first_result.source_type), "æ•°æ®æºç±»å‹ç±»å‹ä¸ä¸€è‡´"
                
                # éªŒè¯ï¼šæ‰€æœ‰ç»“æœéƒ½æœ‰ç›¸åŒçš„æ•°æ®æºä¿¡æ¯
                assert result.source_name == first_result.source_name, "æ•°æ®æºåç§°ä¸ä¸€è‡´"
                assert result.source_type == first_result.source_type, "æ•°æ®æºç±»å‹ä¸ä¸€è‡´"
    
    @given(tweet_data=x_tweet_with_variations())
    @settings(max_examples=30, deadline=None)
    def test_content_item_validation_after_parsing(self, tweet_data):
        """
        å±æ€§æµ‹è¯•ï¼šè§£æåContentIteméªŒè¯çš„å®Œæ•´æ€§
        
        éªŒè¯è§£æç”Ÿæˆçš„ContentItemå¯¹è±¡èƒ½å¤Ÿé€šè¿‡æ•°æ®éªŒè¯
        """
        tweet_raw, expected_data = tweet_data
        
        # è§£ææ¨æ–‡
        result = self.crawler.parse_tweet(tweet_raw)
        
        # éªŒè¯ï¼šContentItemå¯¹è±¡èƒ½å¤Ÿé€šè¿‡éªŒè¯
        try:
            result.validate()
        except ValueError as e:
            pytest.fail(f"è§£æç”Ÿæˆçš„ContentIteméªŒè¯å¤±è´¥: {e}")
        
        # éªŒè¯ï¼šå¯ä»¥åºåˆ—åŒ–å’Œååºåˆ—åŒ–
        try:
            json_str = result.to_json()
            restored = ContentItem.from_json(json_str)
            assert restored.title == result.title, "åºåˆ—åŒ–åæ ‡é¢˜ä¸ä¸€è‡´"
            assert restored.content == result.content, "åºåˆ—åŒ–åå†…å®¹ä¸ä¸€è‡´"
            assert restored.url == result.url, "åºåˆ—åŒ–åURLä¸ä¸€è‡´"
            assert restored.publish_time == result.publish_time, "åºåˆ—åŒ–åæ—¶é—´ä¸ä¸€è‡´"
        except Exception as e:
            pytest.fail(f"ContentItemåºåˆ—åŒ–/ååºåˆ—åŒ–å¤±è´¥: {e}")
    
    @given(
        tweets=st.lists(x_tweet_with_variations(), min_size=1, max_size=3),
        time_window=st.integers(min_value=1, max_value=48)
    )
    @settings(max_examples=20, deadline=None)
    def test_time_window_filtering_completeness(self, tweets, time_window):
        """
        å±æ€§æµ‹è¯•ï¼šæ—¶é—´çª—å£è¿‡æ»¤çš„å®Œæ•´æ€§
        
        éªŒè¯æ—¶é—´çª—å£è¿‡æ»¤åŠŸèƒ½çš„æ­£ç¡®æ€§
        """
        crawler = XCrawler(
            ct0="mock_ct0_token",
            auth_token="mock_auth_token",
            time_window_hours=time_window
        )
        
        # è§£ææ‰€æœ‰æ¨æ–‡
        all_results = []
        expected_in_window = []
        
        for tweet_raw, expected_data in tweets:
            try:
                result = crawler.parse_tweet(tweet_raw)
                all_results.append(result)
                
                # æ£€æŸ¥æ˜¯å¦åº”è¯¥åœ¨æ—¶é—´çª—å£å†…
                if crawler.is_within_time_window(expected_data["publish_time"]):
                    expected_in_window.append(result)
                    
            except Exception as e:
                pytest.fail(f"æ¨æ–‡è§£æå¤±è´¥: {str(e)}")
        
        # ä½¿ç”¨çˆ¬å–å™¨çš„è¿‡æ»¤æ–¹æ³•
        filtered_results = crawler._filter_by_time_window([
            {"id": result.id.split("_")[-1] if "_" in result.id else "123456789",
             "text": result.content,
             "created_at": result.publish_time.strftime("%a %b %d %H:%M:%S +0000 %Y"),
             "user": {"screen_name": result.title.split(":")[0].replace("@", "")}}
            for result in all_results
        ])
        
        # éªŒè¯ï¼šè¿‡æ»¤ç»“æœæ•°é‡æ­£ç¡®
        assert len(filtered_results) == len(expected_in_window), \
            f"æ—¶é—´çª—å£è¿‡æ»¤ç»“æœæ•°é‡ä¸åŒ¹é…ï¼šæœŸæœ› {len(expected_in_window)}ï¼Œå®é™… {len(filtered_results)}"
        
        # éªŒè¯ï¼šæ‰€æœ‰è¿‡æ»¤åçš„ç»“æœéƒ½åœ¨æ—¶é—´çª—å£å†…
        for result in filtered_results:
            assert crawler.is_within_time_window(result.publish_time), \
                "è¿‡æ»¤åçš„ç»“æœåº”è¯¥éƒ½åœ¨æ—¶é—´çª—å£å†…"


if __name__ == "__main__":
    # è¿è¡Œå±æ€§æµ‹è¯•
    pytest.main([__file__, "-v", "--tb=short"])