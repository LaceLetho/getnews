"""
LLMåˆ†æå™¨

å®ç°å››æ­¥åˆ†ææµç¨‹ï¼š
1. è·å–å¸‚åœºå¿«ç…§
2. åˆå¹¶æç¤ºè¯ï¼ˆæ³¨æ„å¸‚åœºå¿«ç…§ä¸­çš„è¶…é“¾æ¥éƒ¨åˆ†ä¸è¦åˆå¹¶ï¼‰
3. ç»“æ„åŒ–è¾“å‡º
4. æ‰¹é‡åˆ†æ

æ”¯æŒåŠ¨æ€åˆ†ç±»ï¼Œä¸ç¡¬ç¼–ç å…·ä½“ç±»åˆ«ã€‚
"""

import json
import logging
import os
from typing import List, Dict, Any, Optional, Union
from datetime import datetime
from pathlib import Path

from ..models import ContentItem, AnalysisResult, StorageConfig
from ..utils.timezone_utils import format_datetime_utc8
from .market_snapshot_service import MarketSnapshotService, MarketSnapshot
from .structured_output_manager import (
    StructuredOutputManager,
    StructuredAnalysisResult,
    BatchAnalysisResult
)
from ..storage.cache_manager import SentMessageCacheManager

try:
    from openai import OpenAI
except ImportError:
    OpenAI = None


class LLMAnalyzer:
    """
    LLMåˆ†æå™¨
    
    å®ç°å››æ­¥åˆ†ææµç¨‹ï¼š
    1. ç¬¬ä¸€æ­¥ï¼šè·å–å¸‚åœºå¿«ç…§
    2. ç¬¬äºŒæ­¥ï¼šåˆå¹¶æç¤ºè¯ï¼ˆæ³¨æ„å¸‚åœºå¿«ç…§ä¸­çš„è¶…é“¾æ¥éƒ¨åˆ†ä¸è¦åˆå¹¶ï¼‰
    3. ç¬¬ä¸‰æ­¥ï¼šç»“æ„åŒ–è¾“å‡º
    4. ç¬¬å››æ­¥ï¼šæ‰¹é‡åˆ†æ
    
    æ”¯æŒåŠ¨æ€åˆ†ç±»ï¼Œä¸ç¡¬ç¼–ç å…·ä½“ç±»åˆ«ã€‚
    """
    
    def __init__(
        self,
        api_key: Optional[str] = None,
        GROK_API_KEY: Optional[str] = None,
        model: str = "gpt-4",
        summary_model: str = "grok-beta",
        market_prompt_path: str = "./prompts/market_summary_prompt.md",
        analysis_prompt_path: str = "./prompts/analysis_prompt.md",
        temperature: float = 0.1,
        max_tokens: int = 4000,
        batch_size: int = 10,
        cache_ttl_minutes: int = 30,
        mock_mode: bool = False,
        cache_manager: Optional[SentMessageCacheManager] = None,
        storage_config: Optional[StorageConfig] = None
    ):
        """
        åˆå§‹åŒ–LLMåˆ†æå™¨
        
        Args:
            api_key: LLM APIå¯†é’¥ï¼Œå¦‚æœä¸ºNoneåˆ™ä»ç¯å¢ƒå˜é‡è¯»å–
            GROK_API_KEY: Grok APIå¯†é’¥ï¼Œç”¨äºå¸‚åœºå¿«ç…§
            model: åˆ†æä½¿ç”¨çš„æ¨¡å‹åç§°
            summary_model: å¸‚åœºå¿«ç…§ä½¿ç”¨çš„æ¨¡å‹åç§°
            market_prompt_path: å¸‚åœºå¿«ç…§æç¤ºè¯è·¯å¾„
            analysis_prompt_path: åˆ†ææç¤ºè¯è·¯å¾„
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            batch_size: æ‰¹é‡åˆ†æçš„æ‰¹æ¬¡å¤§å°
            cache_ttl_minutes: ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆåˆ†é’Ÿï¼‰
            mock_mode: æ˜¯å¦ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼ï¼ˆç”¨äºæµ‹è¯•ï¼‰
            cache_manager: å·²å‘é€æ¶ˆæ¯ç¼“å­˜ç®¡ç†å™¨ï¼ˆå¯é€‰ï¼‰
            storage_config: å­˜å‚¨é…ç½®ï¼ˆç”¨äºåˆ›å»ºç¼“å­˜ç®¡ç†å™¨ï¼Œå¯é€‰ï¼‰
        """
        self.api_key = api_key or os.getenv('LLM_API_KEY', '')
        self.GROK_API_KEY = GROK_API_KEY or os.getenv('GROK_API_KEY', '')
        self.model = model
        self.summary_model = summary_model
        self.market_prompt_path = Path(market_prompt_path)
        self.analysis_prompt_path = Path(analysis_prompt_path)
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.batch_size = batch_size
        self.cache_ttl_minutes = cache_ttl_minutes
        self.mock_mode = mock_mode
        self.logger = logging.getLogger(__name__)
        
        # åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨
        self.cache_manager = cache_manager
        if not self.cache_manager and storage_config:
            try:
                self.cache_manager = SentMessageCacheManager(storage_config)
                self.logger.info("å·²åˆ›å»ºç¼“å­˜ç®¡ç†å™¨å®ä¾‹")
            except Exception as e:
                self.logger.warning(f"åˆ›å»ºç¼“å­˜ç®¡ç†å™¨å¤±è´¥: {e}")
        
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        self.client = None
        if not mock_mode and self.api_key and OpenAI:
            try:
                # æ ¹æ®æ¨¡å‹åˆ¤æ–­ä½¿ç”¨å“ªä¸ªAPI endpoint
                if "minimax" in self.model.lower():
                    # MiniMax API
                    self.client = OpenAI(
                        api_key=self.api_key,
                        base_url="https://api.minimax.chat/v1"
                    )
                elif "grok" in self.model.lower():
                    # xAI Grok API
                    self.client = OpenAI(
                        api_key=self.api_key,
                        base_url="https://api.x.ai/v1"
                    )
                else:
                    # æ ‡å‡† OpenAI API
                    self.client = OpenAI(api_key=self.api_key)
            except Exception as e:
                self.logger.error(f"åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯å¤±è´¥: {e}")
        
        # åˆå§‹åŒ–å¸‚åœºå¿«ç…§æœåŠ¡
        self.market_snapshot_service = MarketSnapshotService(
            GROK_API_KEY=self.GROK_API_KEY,
            summary_model=self.summary_model,
            cache_ttl_minutes=self.cache_ttl_minutes,
            mock_mode=mock_mode
        )
        
        # åˆå§‹åŒ–ç»“æ„åŒ–è¾“å‡ºç®¡ç†å™¨
        self.structured_output_manager = StructuredOutputManager(library="instructor")
        
        # å¦‚æœæœ‰å®¢æˆ·ç«¯ï¼Œè®¾ç½®instructorå®¢æˆ·ç«¯
        if self.client and not mock_mode:
            try:
                self.structured_output_manager.setup_instructor_client(self.client)
            except Exception as e:
                self.logger.warning(f"è®¾ç½®instructorå®¢æˆ·ç«¯å¤±è´¥: {e}")
        
        # ç¼“å­˜çš„å¸‚åœºå¿«ç…§å’Œç³»ç»Ÿæç¤ºè¯
        self._cached_market_snapshot: Optional[MarketSnapshot] = None
        self._cached_system_prompt: Optional[str] = None
        
        if mock_mode:
            self.logger.info("LLMåˆ†æå™¨è¿è¡Œåœ¨æ¨¡æ‹Ÿæ¨¡å¼")
        elif not self.api_key:
            self.logger.warning("æœªæä¾›LLM APIå¯†é’¥")
    
    def analyze_content_batch(
        self,
        items: List[ContentItem],
        use_cached_snapshot: bool = True
    ) -> List[StructuredAnalysisResult]:
        """
        æ‰¹é‡åˆ†æå†…å®¹ï¼ˆå››æ­¥æµç¨‹ï¼‰
        
        Args:
            items: å†…å®¹é¡¹åˆ—è¡¨
            use_cached_snapshot: æ˜¯å¦ä½¿ç”¨ç¼“å­˜çš„å¸‚åœºå¿«ç…§
            
        Returns:
            ç»“æ„åŒ–åˆ†æç»“æœåˆ—è¡¨
        """
        if not items:
            self.logger.info("æ²¡æœ‰å†…å®¹éœ€è¦åˆ†æ")
            return []
        
        try:
            # ç¬¬ä¸€æ­¥ï¼šè·å–å¸‚åœºå¿«ç…§
            market_snapshot = self.get_market_snapshot(use_cached=use_cached_snapshot)
            self.logger.info(f"å¸‚åœºå¿«ç…§æ¥æº: {market_snapshot.source}")
            
            # ç¬¬äºŒæ­¥ï¼šåˆå¹¶æç¤ºè¯ï¼ˆæ³¨æ„å¸‚åœºå¿«ç…§ä¸­çš„è¶…é“¾æ¥éƒ¨åˆ†ä¸è¦åˆå¹¶ï¼‰
            system_prompt = self.merge_prompts_with_snapshot(market_snapshot)
            self.logger.info(f"ç³»ç»Ÿæç¤ºè¯é•¿åº¦: {len(system_prompt)} å­—ç¬¦")
            
            # ç¬¬ä¸‰æ­¥å’Œç¬¬å››æ­¥ï¼šä½¿ç”¨ç»“æ„åŒ–è¾“å‡ºè¿›è¡Œæ‰¹é‡åˆ†æ
            results = self._analyze_batch_with_structured_output(
                items, system_prompt
            )
            
            self.logger.info(f"æ‰¹é‡åˆ†æå®Œæˆï¼Œè¿”å› {len(results)} æ¡ç»“æœ")
            return results
            
        except Exception as e:
            self.logger.error(f"æ‰¹é‡åˆ†æå¤±è´¥: {e}")
            raise
    
    def get_market_snapshot(self, use_cached: bool = True) -> MarketSnapshot:
        """
        ç¬¬ä¸€æ­¥ï¼šè·å–å¸‚åœºå¿«ç…§
        
        Args:
            use_cached: æ˜¯å¦ä½¿ç”¨ç¼“å­˜çš„å¿«ç…§
            
        Returns:
            å¸‚åœºå¿«ç…§å¯¹è±¡
        """
        # å¦‚æœå…è®¸ä½¿ç”¨ç¼“å­˜ä¸”æœ‰ç¼“å­˜ï¼Œç›´æ¥è¿”å›
        if use_cached and self._cached_market_snapshot:
            self.logger.info("ä½¿ç”¨å†…å­˜ç¼“å­˜çš„å¸‚åœºå¿«ç…§")
            return self._cached_market_snapshot
        
        # è¯»å–å¸‚åœºå¿«ç…§æç¤ºè¯æ¨¡æ¿
        prompt_template = self._load_market_prompt_template()
        
        # è°ƒç”¨å¸‚åœºå¿«ç…§æœåŠ¡è·å–å¿«ç…§
        snapshot = self.market_snapshot_service.get_market_snapshot(prompt_template)
        
        # ç¼“å­˜å¿«ç…§
        self._cached_market_snapshot = snapshot
        
        return snapshot
    
    def merge_prompts_with_snapshot(self, market_snapshot: MarketSnapshot) -> str:
        """
        ç¬¬äºŒæ­¥ï¼šåˆå¹¶æç¤ºè¯ï¼ˆæ³¨æ„å¸‚åœºå¿«ç…§ä¸­çš„è¶…é“¾æ¥éƒ¨åˆ†ä¸è¦åˆå¹¶ï¼‰
        
        Args:
            market_snapshot: å¸‚åœºå¿«ç…§å¯¹è±¡
            
        Returns:
            åˆå¹¶åçš„ç³»ç»Ÿæç¤ºè¯
        """
        # è¯»å–åˆ†ææç¤ºè¯æ¨¡æ¿
        analysis_template = self._load_analysis_prompt_template()
        
        # å°†å¸‚åœºå¿«ç…§å†…å®¹æ’å…¥åˆ°åˆ†ææç¤ºè¯ä¸­
        # æŸ¥æ‰¾ ${Grok_Summary_Here} å ä½ç¬¦å¹¶æ›¿æ¢
        system_prompt = analysis_template.replace(
            "${Grok_Summary_Here}",
            market_snapshot.content
        )
        
        # æ›¿æ¢ ${outdated_news} å ä½ç¬¦ï¼ˆæœ€è¿‘12å°æ—¶ï¼‰
        outdated_news = self._get_formatted_cached_messages(hours=12)
        system_prompt = system_prompt.replace(
            "${outdated_news}",
            outdated_news
        )
        
        # ç¼“å­˜ç³»ç»Ÿæç¤ºè¯
        self._cached_system_prompt = system_prompt
        
        return system_prompt
    
    def _get_formatted_cached_messages(self, hours: int = 12) -> str:
        """
        è·å–æ ¼å¼åŒ–çš„ç¼“å­˜æ¶ˆæ¯
        
        Args:
            hours: æ—¶é—´èŒƒå›´ï¼ˆå°æ—¶ï¼‰ï¼Œé»˜è®¤12å°æ—¶
        
        Returns:
            æ ¼å¼åŒ–åçš„ç¼“å­˜æ¶ˆæ¯æ–‡æœ¬ï¼Œå¦‚æœæ²¡æœ‰ç¼“å­˜ç®¡ç†å™¨æˆ–ç¼“å­˜ä¸ºç©ºåˆ™è¿”å›"æ— "
        """
        if not self.cache_manager:
            self.logger.debug("æœªé…ç½®ç¼“å­˜ç®¡ç†å™¨ï¼ŒOutdated Newså°†æ˜¾ç¤º'æ— '")
            return "æ— "
        
        try:
            formatted_messages = self.cache_manager.format_cached_messages_for_prompt(hours=hours)
            if formatted_messages == "æ— ":
                self.logger.info(f"è¿‡å»{hours}å°æ—¶å†…æ²¡æœ‰å·²å‘é€çš„æ¶ˆæ¯ç¼“å­˜ï¼ŒOutdated Newsæ˜¾ç¤º'æ— '")
            else:
                self.logger.info(f"å·²è·å–æ ¼å¼åŒ–çš„ç¼“å­˜æ¶ˆæ¯ï¼ŒåŒ…å«è¿‡å»{hours}å°æ—¶çš„å·²å‘é€å†…å®¹")
            return formatted_messages
        except Exception as e:
            self.logger.warning(f"è·å–ç¼“å­˜æ¶ˆæ¯å¤±è´¥: {e}ï¼ŒOutdated Newså°†æ˜¾ç¤º'æ— '")
            return "æ— "
    
    def _analyze_batch_with_structured_output(
        self,
        items: List[ContentItem],
        system_prompt: str
    ) -> List[StructuredAnalysisResult]:
        """
        ç¬¬ä¸‰æ­¥å’Œç¬¬å››æ­¥ï¼šä½¿ç”¨ç»“æ„åŒ–è¾“å‡ºè¿›è¡Œæ‰¹é‡åˆ†æ
        
        Args:
            items: å†…å®¹é¡¹åˆ—è¡¨
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            
        Returns:
            ç»“æ„åŒ–åˆ†æç»“æœåˆ—è¡¨
        """
        if self.mock_mode:
            return self._generate_mock_results(items)
        
        if not self.client:
            raise RuntimeError("OpenAIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
        
        all_results = []
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(items), self.batch_size):
            batch = items[i:i + self.batch_size]
            self.logger.info(f"å¤„ç†æ‰¹æ¬¡ {i // self.batch_size + 1}ï¼ŒåŒ…å« {len(batch)} æ¡å†…å®¹")
            
            try:
                # æ„å»ºç”¨æˆ·æç¤ºè¯
                user_prompt = self._build_user_prompt(batch)
                
                # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
                messages = [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ]
                
                # ä»¥ç”¨æˆ·å‹å¥½çš„æ–¹å¼æ‰“å°æœ€ç»ˆå‘é€ç»™LLMçš„å®Œæ•´æç¤ºè¯
                self._log_final_prompt(system_prompt, user_prompt, i // self.batch_size + 1)
                
                # ä½¿ç”¨ç»“æ„åŒ–è¾“å‡ºç®¡ç†å™¨å¼ºåˆ¶è¿”å›ç»“æ„åŒ–æ•°æ®ï¼ˆå¯ç”¨web_searchå·¥å…·ï¼‰
                batch_result = self.structured_output_manager.force_structured_response(
                    llm_client=self.client,
                    messages=messages,
                    model=self.model,
                    max_retries=3,
                    temperature=self.temperature,
                    batch_mode=True,
                    enable_web_search=True  # å¯ç”¨web_searchå·¥å…·
                )
                
                # æ‰“å°LLMè¿”å›çš„åŸå§‹æ•°æ®
                self._log_llm_response(batch_result, i // self.batch_size + 1)
                
                # æå–ç»“æœ
                if isinstance(batch_result, BatchAnalysisResult):
                    all_results.extend(batch_result.results)
                    self.logger.info(f"æ‰¹æ¬¡è¿”å› {len(batch_result.results)} æ¡ç»“æœ")
                else:
                    self.logger.warning(f"æ‰¹æ¬¡è¿”å›æ ¼å¼å¼‚å¸¸: {type(batch_result)}")
                
            except Exception as e:
                self.logger.error(f"æ‰¹æ¬¡åˆ†æå¤±è´¥: {e}")
                # ç»§ç»­å¤„ç†ä¸‹ä¸€æ‰¹æ¬¡
                continue
        
        return all_results
    
    def _build_user_prompt(self, items: List[ContentItem]) -> str:
        """
        æ„å»ºç”¨æˆ·æç¤ºè¯ï¼ˆæ‰¹é‡å†…å®¹ï¼‰
        
        Args:
            items: å†…å®¹é¡¹åˆ—è¡¨
            
        Returns:
            ç”¨æˆ·æç¤ºè¯å­—ç¬¦ä¸²
        """
        from email.utils import format_datetime
        from ..utils.timezone_utils import convert_to_utc8
        
        prompt_parts = ["è¯·åˆ†æä»¥ä¸‹æ–°é—»å’Œç¤¾äº¤åª’ä½“å†…å®¹ï¼š\n"]
        
        for i, item in enumerate(items, 1):
            prompt_parts.append(f"\n--- å†…å®¹ {i} ---")
            prompt_parts.append(f"æ ‡é¢˜: {item.title}")
            prompt_parts.append(f"å†…å®¹: {item.content}")
            prompt_parts.append(f"æ¥æº: {item.url}")
            
            # å°† datetime è½¬æ¢ä¸º RFC 2822 æ ¼å¼ï¼ˆå¸¦æ—¶åŒºä¿¡æ¯ï¼‰
            dt_with_tz = convert_to_utc8(item.publish_time)
            rfc2822_time = format_datetime(dt_with_tz)
            prompt_parts.append(f"å‘å¸ƒæ—¶é—´: {rfc2822_time}")
        
        prompt_parts.append("\n\nè¯·æŒ‰ç…§è¦æ±‚è¾“å‡ºJSONæ ¼å¼çš„åˆ†æç»“æœã€‚")
        
        return "\n".join(prompt_parts)
    def _log_final_prompt(self, system_prompt: str, user_prompt: str, batch_number: int) -> None:
        """
        ä»¥ç”¨æˆ·å‹å¥½çš„æ–¹å¼æ‰“å°æœ€ç»ˆå‘é€ç»™LLMçš„å®Œæ•´æç¤ºè¯

        Args:
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            user_prompt: ç”¨æˆ·æç¤ºè¯
            batch_number: æ‰¹æ¬¡ç¼–å·
        """
        separator = "=" * 80

        self.logger.info(f"\n{separator}")
        self.logger.info(f"ğŸ“¤ å‘é€ç»™LLMçš„å®Œæ•´æç¤ºè¯ (æ‰¹æ¬¡ {batch_number})")
        self.logger.info(f"{separator}\n")

        # æ‰“å°ç³»ç»Ÿæç¤ºè¯
        self.logger.info("ğŸ¤– ç³»ç»Ÿæç¤ºè¯ (System Prompt):")
        self.logger.info(f"{'-' * 80}")
        # æˆªæ–­è¿‡é•¿çš„ç³»ç»Ÿæç¤ºè¯ï¼Œåªæ˜¾ç¤ºå‰500å­—ç¬¦å’Œå200å­—ç¬¦
        if len(system_prompt) > 700:
            self.logger.info(f"{system_prompt[:500]}\n\n... [ä¸­é—´çœç•¥ {len(system_prompt) - 700} å­—ç¬¦] ...\n\n{system_prompt[-200:]}")
        else:
            self.logger.info(system_prompt)
        self.logger.info(f"{'-' * 80}\n")

        # æ‰“å°ç”¨æˆ·æç¤ºè¯
        self.logger.info("ğŸ‘¤ ç”¨æˆ·æç¤ºè¯ (User Prompt):")
        self.logger.info(f"{'-' * 80}")
        # æˆªæ–­è¿‡é•¿çš„ç”¨æˆ·æç¤ºè¯ï¼Œåªæ˜¾ç¤ºå‰1000å­—ç¬¦å’Œå300å­—ç¬¦
        if len(user_prompt) > 1300:
            self.logger.info(f"{user_prompt[:1000]}\n\n... [ä¸­é—´çœç•¥ {len(user_prompt) - 1300} å­—ç¬¦] ...\n\n{user_prompt[-300:]}")
        else:
            self.logger.info(user_prompt)
        self.logger.info(f"{'-' * 80}\n")

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        self.logger.info("ğŸ“Š æç¤ºè¯ç»Ÿè®¡:")
        self.logger.info(f"  â€¢ ç³»ç»Ÿæç¤ºè¯é•¿åº¦: {len(system_prompt)} å­—ç¬¦")
        self.logger.info(f"  â€¢ ç”¨æˆ·æç¤ºè¯é•¿åº¦: {len(user_prompt)} å­—ç¬¦")
        self.logger.info(f"  â€¢ æ€»é•¿åº¦: {len(system_prompt) + len(user_prompt)} å­—ç¬¦")
        self.logger.info(f"  â€¢ æ¨¡å‹: {self.model}")
        self.logger.info(f"  â€¢ æ¸©åº¦: {self.temperature}")
        self.logger.info(f"{separator}\n")
    def _log_llm_response(self, response: Any, batch_number: int) -> None:
        """
        ä»¥ç”¨æˆ·å‹å¥½çš„æ–¹å¼æ‰“å°LLMè¿”å›çš„åŸå§‹æ•°æ®

        Args:
            response: LLMè¿”å›çš„å“åº”å¯¹è±¡
            batch_number: æ‰¹æ¬¡ç¼–å·
        """
        import json
        from pydantic import BaseModel

        separator = "=" * 80

        self.logger.info(f"\n{separator}")
        self.logger.info(f"ğŸ“¥ LLMè¿”å›çš„åŸå§‹æ•°æ® (æ‰¹æ¬¡ {batch_number})")
        self.logger.info(f"{separator}\n")

        try:
            # å¦‚æœæ˜¯Pydanticæ¨¡å‹ï¼Œè½¬æ¢ä¸ºå­—å…¸
            if isinstance(response, BaseModel):
                response_dict = response.model_dump()
                response_json = json.dumps(response_dict, ensure_ascii=False, indent=2)
            # å¦‚æœå·²ç»æ˜¯å­—å…¸
            elif isinstance(response, dict):
                response_json = json.dumps(response, ensure_ascii=False, indent=2)
            # å…¶ä»–ç±»å‹ï¼Œå°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            else:
                response_json = str(response)

            # æ‰“å°å“åº”æ•°æ®
            self.logger.info("ğŸ” å“åº”å†…å®¹:")
            self.logger.info(f"{'-' * 80}")

            # å¦‚æœå“åº”å¤ªé•¿ï¼Œè¿›è¡Œæ™ºèƒ½æˆªæ–­
            if len(response_json) > 3000:
                self.logger.info(f"{response_json[:2000]}\n\n... [ä¸­é—´çœç•¥ {len(response_json) - 3000} å­—ç¬¦] ...\n\n{response_json[-1000:]}")
            else:
                self.logger.info(response_json)

            self.logger.info(f"{'-' * 80}\n")

            # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
            self.logger.info("ğŸ“Š å“åº”ç»Ÿè®¡:")
            self.logger.info(f"  â€¢ å“åº”ç±»å‹: {type(response).__name__}")
            self.logger.info(f"  â€¢ å“åº”é•¿åº¦: {len(response_json)} å­—ç¬¦")

            # å¦‚æœæ˜¯BatchAnalysisResultï¼Œæ˜¾ç¤ºç»“æœæ•°é‡
            if hasattr(response, 'results'):
                self.logger.info(f"  â€¢ åˆ†æç»“æœæ•°é‡: {len(response.results)}")

        except Exception as e:
            self.logger.error(f"æ‰“å°LLMå“åº”å¤±è´¥: {e}")
            self.logger.info(f"åŸå§‹å“åº”å¯¹è±¡: {response}")

        self.logger.info(f"{separator}\n")


    
    def _load_market_prompt_template(self) -> str:
        """åŠ è½½å¸‚åœºå¿«ç…§æç¤ºè¯æ¨¡æ¿"""
        try:
            if not self.market_prompt_path.exists():
                raise FileNotFoundError(f"å¸‚åœºå¿«ç…§æç¤ºè¯æ–‡ä»¶ä¸å­˜åœ¨: {self.market_prompt_path}")
            
            with open(self.market_prompt_path, 'r', encoding='utf-8') as f:
                return f.read().strip()
        except Exception as e:
            self.logger.error(f"åŠ è½½å¸‚åœºå¿«ç…§æç¤ºè¯å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤æç¤ºè¯
            return "è¯·ç®€è¦æè¿°å½“å‰åŠ å¯†è´§å¸å¸‚åœºçš„ç°çŠ¶ã€‚"
    
    def _load_analysis_prompt_template(self) -> str:
        """åŠ è½½åˆ†ææç¤ºè¯æ¨¡æ¿"""
        try:
            if not self.analysis_prompt_path.exists():
                raise FileNotFoundError(f"åˆ†ææç¤ºè¯æ–‡ä»¶ä¸å­˜åœ¨: {self.analysis_prompt_path}")
            
            with open(self.analysis_prompt_path, 'r', encoding='utf-8') as f:
                return f.read().strip()
        except Exception as e:
            self.logger.error(f"åŠ è½½åˆ†ææç¤ºè¯å¤±è´¥: {e}")
            raise
    
    def _generate_mock_results(self, items: List[ContentItem]) -> List[StructuredAnalysisResult]:
        """ç”Ÿæˆæ¨¡æ‹Ÿåˆ†æç»“æœï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
        mock_results = []
        
        categories = ["Whale", "MacroLiquidity", "Regulation", "NewProject", "Arbitrage", "Truth", "MonetarySystem", "MarketTrend"]
        
        for i, item in enumerate(items):
            # æ¨¡æ‹Ÿï¼šåªä¿ç•™éƒ¨åˆ†å†…å®¹ï¼Œå…¶ä»–è¢«è¿‡æ»¤
            if i % 3 == 0:  # æ¯3æ¡ä¿ç•™1æ¡
                mock_results.append(StructuredAnalysisResult(
                    time=item.publish_time.strftime('%Y-%m-%d %H:%M'),
                    category=categories[i % len(categories)],
                    weight_score=50 + (i * 10) % 50,
                    summary=f"æ¨¡æ‹Ÿåˆ†æ: {item.title[:50]}...",
                    source=item.url,
                    related_sources=[
                        f"https://example.com/related/{i}/1",
                        f"https://example.com/related/{i}/2"
                    ]
                ))
        
        return mock_results
    
    def get_dynamic_categories(self, results: List[StructuredAnalysisResult]) -> List[str]:
        """
        ä»åˆ†æç»“æœä¸­æå–åŠ¨æ€åˆ†ç±»
        
        Args:
            results: åˆ†æç»“æœåˆ—è¡¨
            
        Returns:
            åˆ†ç±»åç§°åˆ—è¡¨ï¼ˆå»é‡ï¼‰
        """
        categories = set()
        for result in results:
            if result.category:
                categories.add(result.category)
        
        return sorted(list(categories))
    
    def classify_content_dynamic(
        self,
        content: str,
        market_context: str
    ) -> StructuredAnalysisResult:
        """
        åŠ¨æ€åˆ†ç±»å•æ¡å†…å®¹ï¼ˆç”¨äºç‰¹æ®Šåœºæ™¯ï¼‰
        
        Args:
            content: å†…å®¹æ–‡æœ¬
            market_context: å¸‚åœºä¸Šä¸‹æ–‡
            
        Returns:
            ç»“æ„åŒ–åˆ†æç»“æœ
        """
        if self.mock_mode:
            return StructuredAnalysisResult(
                time=datetime.now().strftime('%Y-%m-%d %H:%M'),
                category="MarketTrend",
                weight_score=75,
                summary=f"æ¨¡æ‹Ÿåˆ†æ: {content[:50]}...",
                source="https://example.com/mock",
                related_sources=[]
            )
        
        if not self.client:
            raise RuntimeError("OpenAIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
        
        # æ„å»ºæ¶ˆæ¯
        messages = [
            {"role": "system", "content": market_context},
            {"role": "user", "content": f"è¯·åˆ†æä»¥ä¸‹å†…å®¹ï¼š\n\n{content}"}
        ]
        
        # ä½¿ç”¨ç»“æ„åŒ–è¾“å‡º
        result = self.structured_output_manager.force_structured_response(
            llm_client=self.client,
            messages=messages,
            model=self.model,
            max_retries=3,
            temperature=self.temperature,
            batch_mode=False
        )
        
        return result
    
    def should_ignore_content(self, content: str) -> bool:
        """
        åˆ¤æ–­å†…å®¹æ˜¯å¦åº”è¯¥è¢«å¿½ç•¥
        
        æ³¨æ„ï¼šåœ¨å››æ­¥æµç¨‹ä¸­ï¼Œè¿‡æ»¤é€»è¾‘ç”±å¤§æ¨¡å‹åœ¨æ‰¹é‡åˆ†ææ—¶å®Œæˆï¼Œ
        æ­¤æ–¹æ³•ä¸»è¦ç”¨äºé¢„å¤„ç†é˜¶æ®µçš„å¿«é€Ÿè¿‡æ»¤ã€‚
        
        Args:
            content: å†…å®¹æ–‡æœ¬
            
        Returns:
            æ˜¯å¦åº”è¯¥å¿½ç•¥
        """
        # åŸºæœ¬çš„é¢„è¿‡æ»¤è§„åˆ™
        if not content or len(content.strip()) < 10:
            return True
        
        # å…¶ä»–è¿‡æ»¤é€»è¾‘ç”±å¤§æ¨¡å‹åœ¨åˆ†ææ—¶å®Œæˆ
        return False
    
    def build_system_prompt(self, market_snapshot: MarketSnapshot) -> str:
        """
        æ„å»ºç³»ç»Ÿæç¤ºè¯ï¼ˆä¾¿æ·æ–¹æ³•ï¼‰
        
        æ­¤æ–¹æ³•ä¼šè‡ªåŠ¨æ›¿æ¢ ${Grok_Summary_Here} å’Œ ${outdated_news} å ä½ç¬¦
        
        Args:
            market_snapshot: å¸‚åœºå¿«ç…§
            
        Returns:
            ç³»ç»Ÿæç¤ºè¯
        """
        return self.merge_prompts_with_snapshot(market_snapshot)
    
    def build_user_prompt(self, items: List[ContentItem]) -> str:
        """
        æ„å»ºç”¨æˆ·æç¤ºè¯ï¼ˆä¾¿æ·æ–¹æ³•ï¼‰
        
        Args:
            items: å†…å®¹é¡¹åˆ—è¡¨
            
        Returns:
            ç”¨æˆ·æç¤ºè¯
        """
        return self._build_user_prompt(items)
    
    def parse_structured_response(self, response: str) -> List[StructuredAnalysisResult]:
        """
        è§£æç»“æ„åŒ–å“åº”ï¼ˆç”¨äºé”™è¯¯æ¢å¤ï¼‰
        
        Args:
            response: å“åº”å­—ç¬¦ä¸²
            
        Returns:
            ç»“æ„åŒ–åˆ†æç»“æœåˆ—è¡¨
        """
        try:
            # å°è¯•ä½¿ç”¨ç»“æ„åŒ–è¾“å‡ºç®¡ç†å™¨æ¢å¤
            result = self.structured_output_manager.handle_malformed_response(
                response, batch_mode=True
            )
            
            if isinstance(result, BatchAnalysisResult):
                return result.results
            else:
                return []
        except Exception as e:
            self.logger.error(f"è§£æç»“æ„åŒ–å“åº”å¤±è´¥: {e}")
            return []
    
    def validate_response_format(self, response: Dict[str, Any]) -> bool:
        """
        éªŒè¯å“åº”æ ¼å¼
        
        Args:
            response: å“åº”å­—å…¸
            
        Returns:
            æ˜¯å¦æœ‰æ•ˆ
        """
        validation_result = self.structured_output_manager.validate_output_structure(response)
        return validation_result.is_valid
    
    def handle_empty_batch_response(self) -> List[StructuredAnalysisResult]:
        """
        å¤„ç†ç©ºæ‰¹æ¬¡å“åº”ï¼ˆæ‰€æœ‰å†…å®¹è¢«è¿‡æ»¤ï¼‰
        
        Returns:
            ç©ºåˆ—è¡¨
        """
        self.logger.info("æ‰¹æ¬¡è¿”å›ç©ºç»“æœï¼Œæ‰€æœ‰å†…å®¹è¢«è¿‡æ»¤")
        return []
    
    def retry_with_fallback_model(
        self,
        items: List[ContentItem],
        error: Exception
    ) -> List[StructuredAnalysisResult]:
        """
        ä½¿ç”¨å¤‡ç”¨æ¨¡å‹é‡è¯•
        
        Args:
            items: å†…å®¹é¡¹åˆ—è¡¨
            error: åŸå§‹é”™è¯¯
            
        Returns:
            åˆ†æç»“æœåˆ—è¡¨
        """
        self.logger.warning(f"ä¸»æ¨¡å‹å¤±è´¥: {error}ï¼Œå°è¯•ä½¿ç”¨å¤‡ç”¨æ¨¡å‹")
        
        # è¿™é‡Œå¯ä»¥å®ç°å¤‡ç”¨æ¨¡å‹é€»è¾‘
        # ç›®å‰è¿”å›ç©ºåˆ—è¡¨
        return []
    
    def clear_cache(self) -> None:
        """æ¸…é™¤ç¼“å­˜çš„å¸‚åœºå¿«ç…§å’Œç³»ç»Ÿæç¤ºè¯"""
        self._cached_market_snapshot = None
        self._cached_system_prompt = None
        self.logger.info("å·²æ¸…é™¤LLMåˆ†æå™¨ç¼“å­˜")
    
    def get_cache_info(self) -> Dict[str, Any]:
        """
        è·å–ç¼“å­˜ä¿¡æ¯
        
        Returns:
            ç¼“å­˜ä¿¡æ¯å­—å…¸
        """
        return {
            "has_cached_snapshot": self._cached_market_snapshot is not None,
            "has_cached_prompt": self._cached_system_prompt is not None,
            "snapshot_source": self._cached_market_snapshot.source if self._cached_market_snapshot else None,
            "snapshot_timestamp": self._cached_market_snapshot.timestamp.isoformat() if self._cached_market_snapshot else None
        }
    
    def update_config(self, **kwargs) -> None:
        """
        æ›´æ–°é…ç½®
        
        Args:
            **kwargs: é…ç½®å‚æ•°
        """
        if "temperature" in kwargs:
            self.temperature = kwargs["temperature"]
        
        if "max_tokens" in kwargs:
            self.max_tokens = kwargs["max_tokens"]
        
        if "batch_size" in kwargs:
            self.batch_size = kwargs["batch_size"]
        
        if "model" in kwargs:
            self.model = kwargs["model"]
        
        self.logger.info("LLMåˆ†æå™¨é…ç½®å·²æ›´æ–°")
